{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 安装和初始化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 安装stepfunction模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: stepfunctions in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (1.0.0.7)\n",
      "Requirement not upgraded as not directly required: sagemaker>=1.42.8 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from stepfunctions) (1.51.4)\n",
      "Requirement not upgraded as not directly required: pyyaml in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from stepfunctions) (5.3.1)\n",
      "Requirement not upgraded as not directly required: boto3>=1.9.213 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from stepfunctions) (1.12.27)\n",
      "Requirement not upgraded as not directly required: scipy>=0.19.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker>=1.42.8->stepfunctions) (1.1.0)\n",
      "Requirement not upgraded as not directly required: numpy>=1.9.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker>=1.42.8->stepfunctions) (1.14.3)\n",
      "Requirement not upgraded as not directly required: packaging>=20.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker>=1.42.8->stepfunctions) (20.1)\n",
      "Requirement not upgraded as not directly required: protobuf3-to-dict>=0.1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker>=1.42.8->stepfunctions) (0.1.5)\n",
      "Requirement not upgraded as not directly required: protobuf>=3.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker>=1.42.8->stepfunctions) (3.6.1)\n",
      "Requirement not upgraded as not directly required: importlib-metadata>=1.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker>=1.42.8->stepfunctions) (1.5.0)\n",
      "Requirement not upgraded as not directly required: smdebug-rulesconfig==0.1.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker>=1.42.8->stepfunctions) (0.1.2)\n",
      "Requirement not upgraded as not directly required: s3transfer<0.4.0,>=0.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3>=1.9.213->stepfunctions) (0.3.3)\n",
      "Requirement not upgraded as not directly required: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3>=1.9.213->stepfunctions) (0.9.4)\n",
      "Requirement not upgraded as not directly required: botocore<1.16.0,>=1.15.27 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3>=1.9.213->stepfunctions) (1.15.27)\n",
      "Requirement not upgraded as not directly required: pyparsing>=2.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from packaging>=20.0->sagemaker>=1.42.8->stepfunctions) (2.2.0)\n",
      "Requirement not upgraded as not directly required: six in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from packaging>=20.0->sagemaker>=1.42.8->stepfunctions) (1.11.0)\n",
      "Requirement not upgraded as not directly required: setuptools in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from protobuf>=3.1->sagemaker>=1.42.8->stepfunctions) (39.1.0)\n",
      "Requirement not upgraded as not directly required: zipp>=0.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from importlib-metadata>=1.4.0->sagemaker>=1.42.8->stepfunctions) (3.0.0)\n",
      "Requirement not upgraded as not directly required: urllib3<1.26,>=1.20; python_version != \"3.4\" in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore<1.16.0,>=1.15.27->boto3>=1.9.213->stepfunctions) (1.23)\n",
      "Requirement not upgraded as not directly required: docutils<0.16,>=0.10 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore<1.16.0,>=1.15.27->boto3>=1.9.213->stepfunctions) (0.14)\n",
      "Requirement not upgraded as not directly required: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore<1.16.0,>=1.15.27->boto3>=1.9.213->stepfunctions) (2.7.3)\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install --upgrade stepfunctions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 初始化一些参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import logging\n",
    "import stepfunctions\n",
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "# 通用的初始化\n",
    "session = sagemaker.Session()\n",
    "stepfunctions.set_stream_logger(level=logging.INFO)\n",
    "\n",
    "bucket = 'sagemaker-pipeline-mnist-datasets' # 整个实验要使用的bucket\n",
    "source_prefix = 'source/tf-mnist' # 源数据存放的prefix\n",
    "output_prefix = 'output/tf-mnist' # 转换完的数据存放的prefix\n",
    "\n",
    "# 生成uuid，用于唯一化各个组件需要用到的name\n",
    "id = uuid.uuid4().hex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分配对应的权限：\n",
    "#### 1.给notebook的role分配权限，使其可以创建step function的各个组件\n",
    "给sagemaker notebook的role增加`AWSStepFunctionsFullAccess`权限，以便可以在notebook中创建step function的工作流\n",
    "#### 2.给notebook的role分配权限，使其可以创建Glue Job，可以创建lambda函数，并且可以将role pass给某个lambda函数\n",
    "- 找到notebook的Role -> Permission -> 选择某条策略 -> edit policy\n",
    "- Add additional Policy -> Service选择**IAM** -> Action选择**PassRole** -> Resource选择**Specific** -> 指定可被Pass的role为**query_training_status-role**\n",
    "- 继续 Add additional Policy -> Service选择**Lambda** -> Action选择**Write** -> Resource选择**all resource**\n",
    "- 继续 Add additional Policy -> Service选择**Glue** -> Action选择**Write** -> Resource选择**all resource**\n",
    "- Review and Save changes\n",
    "\n",
    "#### 3.给StepFunction创建IAM Role，使其未来可以具有操作sagemaker的权限\n",
    "- 进入IAM控制台 -> Role -> Create Rule\n",
    "- trusted entity选择**AWS Service** -> 服务选择**Step Function** -> Next Permission\n",
    "- 一路Next直到输入名称`StepFunctionsWorkflowExecutionRole` -> **Create**\n",
    "\n",
    "下面将给这个Role赋予可以操作sagemaker、调用Lambda和EventBridge创建event rules的权限，遵从最佳实践--最小化权限原则\n",
    "\n",
    "- 在Permission下 -> Attach Policies -> Create Policy\n",
    "- 粘贴如下的Policy，并替换必要的变量 [YOUR_NOTEBOOK_ROLE_ARN]， [YOUR_LAMBDA_FUNCTION_PREFIX]， [YOUR_GLUE_ETL_JOB_PREFIX]；由于lambda和glue job的名字有动态的后缀，所以这里只需要定义好前缀。\n",
    "- [YOUR_LAMBDA_FUNCTION_PREFIX] = query-training-status, [YOUR_GLUE_ETL_JOB_PREFIX] = glue-mnist-etl\n",
    "- Review -> 输入名字：StepFunctionsWorkflowExecutionPolicy，并创建Policy\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": \"iam:PassRole\",\n",
    "            \"Resource\": \"[YOUR_NOTEBOOK_ROLE_ARN]\",\n",
    "            \"Condition\": {\n",
    "                \"StringEquals\": {\n",
    "                    \"iam:PassedToService\": \"sagemaker.amazonaws.com\"\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"sagemaker:CreateModel\",\n",
    "                \"sagemaker:DeleteEndpointConfig\",\n",
    "                \"sagemaker:DescribeTrainingJob\",\n",
    "                \"sagemaker:CreateEndpoint\",\n",
    "                \"sagemaker:StopTrainingJob\",\n",
    "                \"sagemaker:CreateTrainingJob\",\n",
    "                \"sagemaker:UpdateEndpoint\",\n",
    "                \"sagemaker:CreateEndpointConfig\",\n",
    "                \"sagemaker:DeleteEndpoint\"\n",
    "            ],\n",
    "            \"Resource\": [\n",
    "                \"arn:aws:sagemaker:*:*:*\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"events:DescribeRule\",\n",
    "                \"events:PutRule\",\n",
    "                \"events:PutTargets\"\n",
    "            ],\n",
    "            \"Resource\": [\n",
    "                \"arn:aws:events:*:*:rule/StepFunctionsGetEventsForSageMakerTrainingJobsRule\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"lambda:InvokeFunction\"\n",
    "            ],\n",
    "            \"Resource\": [\n",
    "                \"arn:aws:lambda:*:*:function:[YOUR_LAMBDA_FUNCTION_PREFIX]*\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"glue:StartJobRun\",\n",
    "                \"glue:GetJobRun\",\n",
    "                \"glue:BatchStopJobRun\",\n",
    "                \"glue:GetJobRuns\"\n",
    "            ],\n",
    "            \"Resource\": \"arn:aws:glue:*:*:job/[YOUR_GLUE_ETL_JOB_PREFIX]*\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "- 然后返回给Role attach policy的窗口，选择刚刚创建的Policy，并attach\n",
    "\n",
    "#### 4.创建Glue Job要使用的Role，这个Role要有Glue Job的要读写数据的Bucket的权限\n",
    "\n",
    "- 进入IAM控制台 -> Roles -> Create Role\n",
    "- trusted entity选择**AWS Service** -> 服务选择**Glue** -> **Next Permission**\n",
    "- 选择 `AmazonS3FullAccess policy`，然后一路next\n",
    "- 直到Review页面，属于名称 `AWS-Glue-S3-Bucket-Access` -> **Create Role**\n",
    "\n",
    "#### 5.创建Lambda函数要使用的Role，函数需要这个role去\n",
    "\n",
    "- 进入IAM控制台 -> Roles -> Create Role\n",
    "- trusted entity选择**AWS Service** -> 服务选择**Lambda** -> **Next Permission**\n",
    "- 选择 `AmazonSageMakerReadOnly` 和 `AWSLambdaBasicExecutionRole`，然后一路next\n",
    "- 直到Review页面，属于名称 `query_training_status-role` -> **Create Role**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 准备数据\n",
    "- 从Internet下载mnist数据集\n",
    "- 对数据集进行处理，将train、validation和test数据集Label和Feature数据分别合并到一个文件中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle, gzip, urllib.request, json\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "urllib.request.urlretrieve(\"http://deeplearning.net/data/mnist/mnist.pkl.gz\", \"mnist.pkl.gz\")\n",
    "with gzip.open('mnist.pkl.gz', 'rb') as f:\n",
    "    train_set, valid_set, test_set = pickle.load(f, encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: (50000, 784) (50000,)\n",
      "Done writing to s3://sagemaker-pipeline-mnist-datasets/source/tf-mnist/train/examples\n",
      "validation: (10000, 784) (10000,)\n",
      "Done writing to s3://sagemaker-pipeline-mnist-datasets/source/tf-mnist/validation/examples\n",
      "test: (10000, 784) (10000,)\n",
      "Done writing to s3://sagemaker-pipeline-mnist-datasets/source/tf-mnist/test/examples\n"
     ]
    }
   ],
   "source": [
    "# 将下载的mnist数据的feature和label合并到一起，并存储到S3\n",
    "def convert_data():\n",
    "    data_partitions = [('train', train_set), ('validation', valid_set), ('test', test_set)]\n",
    "    for data_partition_name, data_partition in data_partitions:\n",
    "        print('{}: {} {}'.format(data_partition_name, data_partition[0].shape, data_partition[1].shape))\n",
    "        labels = [t.tolist() for t in data_partition[1]]\n",
    "        features = [t.tolist() for t in data_partition[0]]\n",
    "        \n",
    "        if data_partition_name != 'test':\n",
    "            examples = np.insert(features, 0, labels, axis=1)  # 在feature矩阵的第0列插入labels\n",
    "        else:\n",
    "            examples = features  # test数据集没有把labels加进去，why？\n",
    "        \n",
    "        np.savetxt('data.csv', examples, delimiter=',')\n",
    "        \n",
    "        key = \"{}/{}/examples\".format(source_prefix,data_partition_name)\n",
    "        url = 's3://{}/{}'.format(bucket, key)\n",
    "        boto3.Session().resource('s3').Bucket(bucket).Object(key).upload_file('data.csv')\n",
    "        print('Done writing to {}'.format(url))\n",
    "        \n",
    "convert_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建资源\n",
    "\n",
    "### 创建Glue ETL Job\n",
    "\n",
    "- 在这里我们创建的Glue ETL Job的作用是对所有数据集的Features进行normalization，以降低极值对训练和预测的影响\n",
    "- glue是一个serverless的etl服务，底层通过spark实现，我们可以编写etl脚本交由glue运行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "IdempotentParameterMismatchException",
     "evalue": "An error occurred (IdempotentParameterMismatchException) when calling the CreateJob operation: Job with name 'train-val-norm-job-85f525994ce84c24a9716697288794de' already submitted with different configuration",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIdempotentParameterMismatchException\u001b[0m      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-db5a3a410a29>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mWorkerType\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Standard'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mNumberOfWorkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mTimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m )\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    314\u001b[0m                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n\u001b[1;32m    315\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0merror_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Code\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 626\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    627\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIdempotentParameterMismatchException\u001b[0m: An error occurred (IdempotentParameterMismatchException) when calling the CreateJob operation: Job with name 'train-val-norm-job-85f525994ce84c24a9716697288794de' already submitted with different configuration"
     ]
    }
   ],
   "source": [
    "from sagemaker.s3 import S3Uploader\n",
    "\n",
    "# 将glue脚本上传到s3\n",
    "glue_script_location = S3Uploader.upload(local_path='./train_val_norm.py',\n",
    "                               desired_s3_uri='s3://{}/{}'.format(bucket, 'glue_script'),\n",
    "                               session=session)\n",
    "\n",
    "job_name = 'train-val-norm-job-{}'.format(id) # 定义glue job的名字\n",
    "glue_role = 'AWS-Glue-S3-Bucket-Access'  # 使用权限设置章节中创建的glue role\n",
    "\n",
    "glue_client = boto3.client('glue')\n",
    "\n",
    "response = glue_client.create_job(\n",
    "    Name=job_name,\n",
    "    Description='PySpark job to normalize the features of train and validation data',\n",
    "    Role=glue_role, \n",
    "    ExecutionProperty={\n",
    "        'MaxConcurrentRuns': 2\n",
    "    },\n",
    "    Command={\n",
    "        'Name': 'glueetl',\n",
    "        'ScriptLocation': glue_script_location,\n",
    "        'PythonVersion': '3'\n",
    "    },\n",
    "    DefaultArguments={\n",
    "        '--job-language': 'python'\n",
    "    },\n",
    "    GlueVersion='1.0',\n",
    "    WorkerType='Standard',\n",
    "    NumberOfWorkers=2,\n",
    "    Timeout=60\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建sagemaker的estimator\n",
    "\n",
    "- estimator是一个对象，用来完成sagemaker的各个环节，training和hosting等\n",
    "- 需要定义estimator的配置，比如训练数据，训练实例类型，超参数等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "# 定义训练配置，实例类型和超参等\n",
    "s3_output_location = 's3://{}/{}/{}'.format(bucket, output_prefix, 'tf-mninst-output')\n",
    "model_dir = '/opt/ml/model'\n",
    "train_instance_type = 'ml.m5.xlarge'\n",
    "hyperparameters = {'epochs': 5, 'batch_size': 128, 'learning_rate': 0.01, 'other_para':0.1}\n",
    "\n",
    "# 如果需要监控训练算法中某一个指标，可以定义metric_definitions并传入Tensorflow estimator，被监控的metrics会被解析并打到cloudwatch\n",
    "metric_definitions = [{'Name': 'accuracy',\n",
    "                       'Regex': 'accuracy=(.*?);'}]\n",
    "\n",
    "# 创建一个tensorflow的estimator\n",
    "tf_estimator = TensorFlow(\n",
    "                       entry_point='my_train_1.py',\n",
    "                       model_dir=model_dir,\n",
    "                       output_path=s3_output_location,\n",
    "                       train_instance_type=train_instance_type,\n",
    "                       train_instance_count=1,\n",
    "                       hyperparameters=hyperparameters,\n",
    "                       role=sagemaker.get_execution_role(),\n",
    "                       base_job_name='tf-scriptmode-mnist',\n",
    "                       framework_version='2.0.0',\n",
    "                       py_version='py3',\n",
    "                       metric_definitions=metric_definitions,\n",
    "                       script_mode=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义Step Function Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### import相关module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stepfunctions\n",
    "from stepfunctions import steps\n",
    "from stepfunctions.steps import TrainingStep, ModelStep\n",
    "from stepfunctions.inputs import ExecutionInput\n",
    "from stepfunctions.workflow import Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义step function的input的schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution_input = ExecutionInput(schema={\n",
    "    'TrainingJobName': str,\n",
    "    'GlueJobName': str,\n",
    "    'ModelName': str,\n",
    "    'EndpointName': str\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义glue step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义要传入glue job的参数\n",
    "source_path = 's3://{}/{}/'.format(bucket, source_prefix)\n",
    "output_path = 's3://{}/{}/'.format(bucket, output_prefix)\n",
    "train_prefix = 'train'\n",
    "val_prefix = 'validation'\n",
    "\n",
    "etl_step = steps.GlueStartJobRunStep(\n",
    "    'Extract, Transform, Load',\n",
    "    parameters={\"JobName\": execution_input['GlueJobName'],\n",
    "                \"Arguments\":{\n",
    "                    '--SOURCE_PATH': source_path,\n",
    "                    '--OUTPUT_PATH': output_path,\n",
    "                    '--TRAIN_PREFIX': train_prefix + '/',\n",
    "                    '--VAL_PREFIX': val_prefix + '/'}\n",
    "               }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义sagemaker training step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义训练数据的位置\n",
    "train_data = 's3://{}/{}/{}'.format(bucket, output_prefix, 'train')\n",
    "validation_data = 's3://{}/{}/{}'.format(bucket, output_prefix, 'validation')\n",
    "\n",
    "# data chennels会作为参数传递给estimator构造函数，定义训练数据的信息\n",
    "data_channels = {'train': train_data, 'validation': validation_data}\n",
    "\n",
    "\n",
    "training_step = steps.TrainingStep(\n",
    "    'Model Training', \n",
    "    estimator=tf_estimator,\n",
    "    data=data_channels,\n",
    "    job_name=execution_input['TrainingJobName'],\n",
    "    wait_for_completion=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义sagemaker生成model的step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_step = steps.ModelStep(\n",
    "    'Save Model',\n",
    "    model=training_step.get_expected_model(),\n",
    "    model_name=execution_input['ModelName'],\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    result_path='$.ModelStepResults'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义部署model的endpoint configure的step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_config_step = steps.EndpointConfigStep(\n",
    "    \"Create Model Endpoint Config\",\n",
    "    endpoint_config_name=execution_input['ModelName'],\n",
    "    model_name=execution_input['ModelName'],\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m4.xlarge'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 创建endpoint step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_step = steps.EndpointStep(\n",
    "    'Update Model Endpoint',\n",
    "    endpoint_name=execution_input['EndpointName'],\n",
    "    endpoint_config_name=execution_input['ModelName'],\n",
    "    update=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 生成workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_definition = steps.Chain([\n",
    "    etl_step,\n",
    "    training_step,\n",
    "    model_step,\n",
    "    endpoint_config_step,\n",
    "    endpoint_step\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用之前创建的step function role\n",
    "workflow_execution_role = 'arn:aws:iam::935206693453:role/StepFunctionsWorkflowExecutionRole'\n",
    "\n",
    "workflow = Workflow(\n",
    "    name='My-SM-Pipline-{}'.format(id),\n",
    "    definition=workflow_definition,\n",
    "    role=workflow_execution_role,\n",
    "    execution_input=execution_input\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://do0of8uwbahzz.cloudfront.net/graph.css\">\n",
       "<div id=\"graph-630\" class=\"workflowgraph\">\n",
       "    \n",
       "    <svg></svg>\n",
       "    \n",
       "</div>\n",
       "\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "require.config({\n",
       "    paths: {\n",
       "        sfn: \"https://do0of8uwbahzz.cloudfront.net/sfn\",\n",
       "    }\n",
       "});\n",
       "\n",
       "require(['sfn'], function(sfn) {\n",
       "    var options = {\n",
       "        width: $('#graph-630').width(),\n",
       "        height: 600,\n",
       "        layout: 'LR',\n",
       "        resizeHeight: true\n",
       "    };\n",
       "\n",
       "    var definition = {\"StartAt\": \"Extract, Transform, Load\", \"States\": {\"Extract, Transform, Load\": {\"Parameters\": {\"JobName.$\": \"$$.Execution.Input['GlueJobName']\", \"Arguments\": {\"--SOURCE_PATH\": \"s3://sagemaker-pipeline-mnist-datasets/source/tf-mnist/\", \"--OUTPUT_PATH\": \"s3://sagemaker-pipeline-mnist-datasets/output/tf-mnist/\", \"--TRAIN_PREFIX\": \"train/\", \"--VAL_PREFIX\": \"validation/\"}}, \"Resource\": \"arn:aws:states:::glue:startJobRun.sync\", \"Type\": \"Task\", \"Next\": \"Model Training\"}, \"Model Training\": {\"Resource\": \"arn:aws:states:::sagemaker:createTrainingJob.sync\", \"Parameters\": {\"AlgorithmSpecification\": {\"TrainingImage\": \"763104351884.dkr.ecr.us-east-2.amazonaws.com/tensorflow-training:2.0.0-cpu-py3\", \"TrainingInputMode\": \"File\"}, \"OutputDataConfig\": {\"S3OutputPath\": \"s3://sagemaker-pipeline-mnist-datasets/output/tf-mnist/tf-mninst-output\"}, \"StoppingCondition\": {\"MaxRuntimeInSeconds\": 86400}, \"ResourceConfig\": {\"InstanceCount\": 1, \"InstanceType\": \"ml.m5.xlarge\", \"VolumeSizeInGB\": 30}, \"RoleArn\": \"arn:aws:iam::935206693453:role/service-role/AmazonSageMaker-ExecutionRole-20191205T151136\", \"InputDataConfig\": [{\"DataSource\": {\"S3DataSource\": {\"S3DataType\": \"S3Prefix\", \"S3Uri\": \"s3://sagemaker-pipeline-mnist-datasets/output/tf-mnist/train\", \"S3DataDistributionType\": \"FullyReplicated\"}}, \"ChannelName\": \"train\"}, {\"DataSource\": {\"S3DataSource\": {\"S3DataType\": \"S3Prefix\", \"S3Uri\": \"s3://sagemaker-pipeline-mnist-datasets/output/tf-mnist/validation\", \"S3DataDistributionType\": \"FullyReplicated\"}}, \"ChannelName\": \"validation\"}], \"HyperParameters\": {\"epochs\": \"5\", \"batch_size\": \"128\", \"learning_rate\": \"0.01\", \"other_para\": \"0.1\", \"sagemaker_submit_directory\": \"\\\"s3://sagemaker-pipeline-mnist-datasets/tf-scriptmode-mnist-2020-04-14-12-05-44-566/source/sourcedir.tar.gz\\\"\", \"sagemaker_program\": \"\\\"my_train_1.py\\\"\", \"sagemaker_enable_cloudwatch_metrics\": \"false\", \"sagemaker_container_log_level\": \"20\", \"sagemaker_job_name\": \"\\\"tf-scriptmode-mnist-2020-04-14-12-05-44-748\\\"\", \"sagemaker_region\": \"\\\"us-east-2\\\"\", \"model_dir\": \"\\\"/opt/ml/model\\\"\"}, \"TrainingJobName.$\": \"$$.Execution.Input['TrainingJobName']\", \"DebugHookConfig\": {\"S3OutputPath\": \"s3://sagemaker-pipeline-mnist-datasets/output/tf-mnist/tf-mninst-output\"}}, \"Type\": \"Task\", \"Next\": \"Save Model\"}, \"Save Model\": {\"ResultPath\": \"$.ModelStepResults\", \"Parameters\": {\"ModelName.$\": \"$$.Execution.Input['ModelName']\", \"PrimaryContainer\": {\"Image\": \"763104351884.dkr.ecr.us-east-2.amazonaws.com/tensorflow-inference:2.0-cpu\", \"Environment\": {\"SAGEMAKER_PROGRAM\": null, \"SAGEMAKER_SUBMIT_DIRECTORY\": null, \"SAGEMAKER_ENABLE_CLOUDWATCH_METRICS\": \"false\", \"SAGEMAKER_CONTAINER_LOG_LEVEL\": \"20\", \"SAGEMAKER_REGION\": \"us-east-2\"}, \"ModelDataUrl.$\": \"$['ModelArtifacts']['S3ModelArtifacts']\"}, \"ExecutionRoleArn\": \"arn:aws:iam::935206693453:role/service-role/AmazonSageMaker-ExecutionRole-20191205T151136\"}, \"Resource\": \"arn:aws:states:::sagemaker:createModel\", \"Type\": \"Task\", \"Next\": \"Create Model Endpoint Config\"}, \"Create Model Endpoint Config\": {\"Resource\": \"arn:aws:states:::sagemaker:createEndpointConfig\", \"Parameters\": {\"EndpointConfigName.$\": \"$$.Execution.Input['ModelName']\", \"ProductionVariants\": [{\"InitialInstanceCount\": 1, \"InstanceType\": \"ml.m4.xlarge\", \"ModelName.$\": \"$$.Execution.Input['ModelName']\", \"VariantName\": \"AllTraffic\"}]}, \"Type\": \"Task\", \"Next\": \"Update Model Endpoint\"}, \"Update Model Endpoint\": {\"Resource\": \"arn:aws:states:::sagemaker:createEndpoint\", \"Parameters\": {\"EndpointConfigName.$\": \"$$.Execution.Input['ModelName']\", \"EndpointName.$\": \"$$.Execution.Input['EndpointName']\"}, \"Type\": \"Task\", \"End\": true}}};\n",
       "    var elementId = '#graph-630';\n",
       "\n",
       "    var graph = new sfn.StateMachineGraph(definition, elementId, options);\n",
       "    graph.render();\n",
       "});\n",
       "\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow.render_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[INFO] Workflow created successfully on AWS Step Functions.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'arn:aws:states:us-east-2:935206693453:stateMachine:My-SM-Pipline-85f525994ce84c24a9716697288794de'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow.create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[INFO] Workflow execution started successfully on AWS Step Functions.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "execution = workflow.execute(\n",
    "    inputs={\n",
    "        'TrainingJobName': 'my-sm-pipeline-job-{}'.format(id), # Each Sagemaker Job requires a unique name,\n",
    "        'GlueJobName': job_name,\n",
    "        'ModelName': 'my-sm-pipeline-model-{}'.format(id),\n",
    "        'EndpointName': 'my-sm-pipeline-endpoint-{}'.format(id)\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
