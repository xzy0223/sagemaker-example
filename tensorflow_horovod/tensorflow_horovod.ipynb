{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个demo的目的是告诉大家，如何在sagemaker使用script模式训练tensorflow模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先我们先准备训练数据，这个例子中我们使用最简单MNIST数据集来进行训练\n",
    "我们首先下载MNIST数据集并且将其解析为numpy array数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "import pickle, gzip, urllib.request, json\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "urllib.request.urlretrieve(\"http://deeplearning.net/data/mnist/mnist.pkl.gz\", \"mnist.pkl.gz\")\n",
    "with gzip.open('mnist.pkl.gz', 'rb') as f:\n",
    "    train_set, valid_set, test_set = pickle.load(f, encoding='latin1')\n",
    "print(valid_set[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后通过matplotlib展示下数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a 5\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 144x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a 0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 144x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a 4\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 144x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a 1\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 144x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a 9\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 144x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a 2\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 144x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a 1\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 144x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a 3\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 144x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a 1\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 144x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a 4\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 144x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (2,10)\n",
    "\n",
    "for i in range(0, 10):\n",
    "    img = train_set[0][i]\n",
    "    label = train_set[1][i]\n",
    "    img_reshape = img.reshape((28,28))\n",
    "    imgplot = plt.imshow(img_reshape, cmap='gray')\n",
    "    print('This is a {}'.format(label))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这一步我们将刚才numpy数据转换成CSV文件并上传到一个S3的存储桶中\n",
    "\n",
    "我们通过prefix的方式组织train、validation和test数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: (50000, 784) (50000,)\n",
      "Done writing to s3://sagemaker-mnist-datasets/tf-mnist/train/examples\n",
      "validation: (10000, 784) (10000,)\n",
      "Done writing to s3://sagemaker-mnist-datasets/tf-mnist/validation/examples\n",
      "test: (10000, 784) (10000,)\n",
      "Done writing to s3://sagemaker-mnist-datasets/tf-mnist/test/examples\n",
      "CPU times: user 31.2 s, sys: 3.62 s, total: 34.8 s\n",
      "Wall time: 41.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import os\n",
    "import boto3\n",
    "import re\n",
    "import copy\n",
    "import time\n",
    "import io\n",
    "import struct\n",
    "from time import gmtime, strftime\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "bucket='sagemaker-mnist-datasets' # Replace with your s3 bucket name\n",
    "prefix = 'tf-mnist' # Used as part of the path in the bucket where you store data\n",
    "bucket_path = 'https://s3-{}.amazonaws.com/{}'.format(region,bucket) # The URL to access the bucket\n",
    "\n",
    "\n",
    "def convert_data():\n",
    "    data_partitions = [('train', train_set), ('validation', valid_set), ('test', test_set)]\n",
    "    for data_partition_name, data_partition in data_partitions:\n",
    "        print('{}: {} {}'.format(data_partition_name, data_partition[0].shape, data_partition[1].shape))\n",
    "        labels = [t.tolist() for t in data_partition[1]]\n",
    "        features = [t.tolist() for t in data_partition[0]]\n",
    "        \n",
    "        if data_partition_name != 'test':\n",
    "            examples = np.insert(features, 0, labels, axis=1)  # 在feature矩阵的第0列插入labels\n",
    "        else:\n",
    "            examples = features  # test数据集没有把labels加进去，why？\n",
    "        #print(examples[50000,:])\n",
    "        \n",
    "        \n",
    "        np.savetxt('data.csv', examples, delimiter=',')\n",
    "        \n",
    "        \n",
    "        \n",
    "        key = \"{}/{}/examples\".format(prefix,data_partition_name)\n",
    "        url = 's3://{}/{}'.format(bucket, key)\n",
    "        boto3.Session().resource('s3').Bucket(bucket).Object(key).upload_file('data.csv')\n",
    "        print('Done writing to {}'.format(url))\n",
    "        \n",
    "convert_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义各个数据集的S3 URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-mnist-datasets/tf-mnist/train\n"
     ]
    }
   ],
   "source": [
    "train_data = 's3://{}/{}/{}'.format(bucket, prefix, 'train')\n",
    "\n",
    "validation_data = 's3://{}/{}/{}'.format(bucket, prefix, 'validation')\n",
    "\n",
    "s3_output_location = 's3://{}/{}/{}'.format(bucket, prefix, 'tf-mninst-output')\n",
    "print(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义用于训练和验证的数据集Channel，一会儿会作为参数传个训练任务"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 's3://sagemaker-mnist-datasets/tf-mnist/train', 'validation': 's3://sagemaker-mnist-datasets/tf-mnist/validation'}\n"
     ]
    }
   ],
   "source": [
    "data_channels = {'train': train_data, 'validation': validation_data}\n",
    "print(data_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在sagemaker上进行分布式训练，需要在创建estimator时指定distributions参数，我们用的是horovod，是基于open mpi技术；此外还需要指定instance_count，当这个数字大于1时，sagemaker会调用mpirun命令进行训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "distributions = {'mpi': {'enabled': True}}\n",
    "instance_count = 2\n",
    "\n",
    "model_dir = '/opt/ml/model'\n",
    "# train_instance_type = 'local'\n",
    "train_instance_type = 'ml.m5.xlarge'\n",
    "# train_instance_type = 'ml.p3.2xlarge'\n",
    "hyperparameters = {'epochs': 20, 'batch_size': 128, 'learning_rate': 0.01, 'other_para':0.1}\n",
    "# 如果需要监控训练算法中某一个指标，可以定义metric_definitions并传入Tensorflow estimator，被监控的metrics会被解析并打到cloudwatch\n",
    "metric_definitions = [{'Name': 'accuracy',\n",
    "                       'Regex': 'accuracy=(.*?);'}]\n",
    "\n",
    "tf_estimator = TensorFlow(\n",
    "                       entry_point='horovod_train.py',\n",
    "                       model_dir=model_dir,\n",
    "                       output_path=s3_output_location,\n",
    "                       train_instance_type=train_instance_type,\n",
    "                       train_instance_count=instance_count,\n",
    "                       distributions=distributions,\n",
    "                       hyperparameters=hyperparameters,\n",
    "                       role=sagemaker.get_execution_role(),\n",
    "                       base_job_name='tf-scriptmode-mnist',\n",
    "                       framework_version='2.0.0',\n",
    "                       py_version='py3',\n",
    "                       metric_definitions=metric_definitions,\n",
    "                       script_mode=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-04 14:12:24 Starting - Starting the training job...\n",
      "2020-03-04 14:12:25 Starting - Launching requested ML instances...\n",
      "2020-03-04 14:13:19 Starting - Preparing the instances for training......\n",
      "2020-03-04 14:14:09 Downloading - Downloading input data...\n",
      "2020-03-04 14:14:47 Training - Training image download completed. Training in progress..\u001b[35m2020-03-04 14:14:50,119 sagemaker-containers INFO     Imported framework sagemaker_tensorflow_container.training\u001b[0m\n",
      "\u001b[35m2020-03-04 14:14:50,124 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m2020-03-04 14:14:50,359 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m2020-03-04 14:14:51,799 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m2020-03-04 14:14:51,811 sagemaker-containers INFO     Starting MPI run as worker node.\u001b[0m\n",
      "\u001b[35m2020-03-04 14:14:51,811 sagemaker-containers INFO     Waiting for MPI Master to create SSH daemon.\u001b[0m\n",
      "\u001b[35m2020-03-04 14:14:51,817 paramiko.transport INFO     Connected (version 2.0, client OpenSSH_7.6p1)\u001b[0m\n",
      "\u001b[35m/usr/local/lib/python3.6/dist-packages/paramiko/kex_ecdh_nist.py:39: CryptographyDeprecationWarning: encode_point has been deprecated on EllipticCurvePublicNumbers and will be removed in a future version. Please use EllipticCurvePublicKey.public_bytes to obtain both compressed and uncompressed point encoding.\n",
      "  m.add_string(self.Q_C.public_numbers().encode_point())\u001b[0m\n",
      "\u001b[35m/usr/local/lib/python3.6/dist-packages/paramiko/kex_ecdh_nist.py:96: CryptographyDeprecationWarning: Support for unsafe construction of public numbers from encoded data will be removed in a future version. Please use EllipticCurvePublicKey.from_encoded_point\n",
      "  self.curve, Q_S_bytes\u001b[0m\n",
      "\u001b[35m/usr/local/lib/python3.6/dist-packages/paramiko/kex_ecdh_nist.py:111: CryptographyDeprecationWarning: encode_point has been deprecated on EllipticCurvePublicNumbers and will be removed in a future version. Please use EllipticCurvePublicKey.public_bytes to obtain both compressed and uncompressed point encoding.\n",
      "  hm.add_string(self.Q_C.public_numbers().encode_point())\u001b[0m\n",
      "\u001b[35m2020-03-04 14:14:51,932 paramiko.transport INFO     Authentication (publickey) successful!\u001b[0m\n",
      "\u001b[35m2020-03-04 14:14:51,933 sagemaker-containers INFO     Can connect to host algo-1\u001b[0m\n",
      "\u001b[35m2020-03-04 14:14:51,934 sagemaker-containers INFO     MPI Master online, creating SSH daemon.\u001b[0m\n",
      "\u001b[35m2020-03-04 14:14:51,941 sagemaker-containers INFO     Waiting for MPI process to finish.\u001b[0m\n",
      "\u001b[34m2020-03-04 14:14:50,044 sagemaker-containers INFO     Imported framework sagemaker_tensorflow_container.training\u001b[0m\n",
      "\u001b[34m2020-03-04 14:14:50,050 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-03-04 14:14:50,290 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-03-04 14:14:51,726 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-03-04 14:14:51,738 sagemaker-containers INFO     Starting MPI run as worker node.\u001b[0m\n",
      "\u001b[34m2020-03-04 14:14:51,738 sagemaker-containers INFO     Creating SSH daemon.\u001b[0m\n",
      "\u001b[34m2020-03-04 14:14:51,743 sagemaker-containers INFO     Waiting for MPI workers to establish their SSH connections\u001b[0m\n",
      "\u001b[34m2020-03-04 14:14:51,744 sagemaker-containers INFO     Cannot connect to host algo-2\u001b[0m\n",
      "\u001b[34m2020-03-04 14:14:51,744 sagemaker-containers INFO     Connection failed with exception: \n",
      " [Errno None] Unable to connect to port 22 on 10.0.222.3\u001b[0m\n",
      "\u001b[34m2020-03-04 14:14:52,750 paramiko.transport INFO     Connected (version 2.0, client OpenSSH_7.6p1)\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/dist-packages/paramiko/kex_ecdh_nist.py:39: CryptographyDeprecationWarning: encode_point has been deprecated on EllipticCurvePublicNumbers and will be removed in a future version. Please use EllipticCurvePublicKey.public_bytes to obtain both compressed and uncompressed point encoding.\n",
      "  m.add_string(self.Q_C.public_numbers().encode_point())\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/dist-packages/paramiko/kex_ecdh_nist.py:96: CryptographyDeprecationWarning: Support for unsafe construction of public numbers from encoded data will be removed in a future version. Please use EllipticCurvePublicKey.from_encoded_point\n",
      "  self.curve, Q_S_bytes\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/dist-packages/paramiko/kex_ecdh_nist.py:111: CryptographyDeprecationWarning: encode_point has been deprecated on EllipticCurvePublicNumbers and will be removed in a future version. Please use EllipticCurvePublicKey.public_bytes to obtain both compressed and uncompressed point encoding.\n",
      "  hm.add_string(self.Q_C.public_numbers().encode_point())\u001b[0m\n",
      "\u001b[34m2020-03-04 14:14:52,858 paramiko.transport INFO     Authentication (publickey) successful!\u001b[0m\n",
      "\u001b[34m2020-03-04 14:14:52,858 sagemaker-containers INFO     Can connect to host algo-2\u001b[0m\n",
      "\u001b[34m2020-03-04 14:14:52,859 sagemaker-containers INFO     Worker algo-2 available for communication\u001b[0m\n",
      "\u001b[34m2020-03-04 14:14:52,859 sagemaker-containers INFO     Env Hosts: ['algo-1', 'algo-2'] Hosts: ['algo-1', 'algo-2'] process_per_hosts: 1 num_processes: 2\u001b[0m\n",
      "\u001b[34m2020-03-04 14:14:52,861 sagemaker-containers INFO     Network interface name: eth0\u001b[0m\n",
      "\u001b[34m2020-03-04 14:14:52,867 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-03-04 14:14:52,877 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_mpi_custom_mpi_options\": \"\",\n",
      "        \"sagemaker_mpi_enabled\": true\n",
      "    },\n",
      "    \"channel_input_dirs\": {\n",
      "        \"validation\": \"/opt/ml/input/data/validation\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_tensorflow_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch_size\": 128,\n",
      "        \"other_para\": 0.1,\n",
      "        \"model_dir\": \"/opt/ml/model\",\n",
      "        \"epochs\": 20,\n",
      "        \"learning_rate\": 0.01\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"validation\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"tf-scriptmode-mnist-2020-03-04-14-12-23-944\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-mnist-datasets/tf-scriptmode-mnist-2020-03-04-14-12-23-944/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"horovod_train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"horovod_train.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"batch_size\":128,\"epochs\":20,\"learning_rate\":0.01,\"model_dir\":\"/opt/ml/model\",\"other_para\":0.1}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=horovod_train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={\"sagemaker_mpi_custom_mpi_options\":\"\",\"sagemaker_mpi_enabled\":true}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"validation\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\",\"validation\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=horovod_train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_tensorflow_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-mnist-datasets/tf-scriptmode-mnist-2020-03-04-14-12-23-944/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_mpi_custom_mpi_options\":\"\",\"sagemaker_mpi_enabled\":true},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\",\"validation\":\"/opt/ml/input/data/validation\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_tensorflow_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"batch_size\":128,\"epochs\":20,\"learning_rate\":0.01,\"model_dir\":\"/opt/ml/model\",\"other_para\":0.1},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"validation\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"tf-scriptmode-mnist-2020-03-04-14-12-23-944\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-mnist-datasets/tf-scriptmode-mnist-2020-03-04-14-12-23-944/source/sourcedir.tar.gz\",\"module_name\":\"horovod_train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"horovod_train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--batch_size\",\"128\",\"--epochs\",\"20\",\"--learning_rate\",\"0.01\",\"--model_dir\",\"/opt/ml/model\",\"--other_para\",\"0.1\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_VALIDATION=/opt/ml/input/data/validation\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH_SIZE=128\u001b[0m\n",
      "\u001b[34mSM_HP_OTHER_PARA=0.1\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=20\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=0.01\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/usr/local/bin:/usr/lib/python36.zip:/usr/lib/python3.6:/usr/lib/python3.6/lib-dynload:/usr/local/lib/python3.6/dist-packages:/usr/lib/python3/dist-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34mmpirun --host algo-1,algo-2 -np 2 --allow-run-as-root --display-map --tag-output -mca btl_tcp_if_include eth0 -mca oob_tcp_if_include eth0 -mca plm_rsh_no_tree_spawn 1 -bind-to socket -map-by slot -mca pml ob1 -mca btl ^openib -mca orte_abort_on_non_zero_status 1 -x NCCL_MIN_NRINGS=4 -x NCCL_SOCKET_IFNAME=eth0 -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH -x PATH -x LD_PRELOAD=/usr/local/lib/python3.6/dist-packages/gethostname.cpython-36m-x86_64-linux-gnu.so -x SM_HOSTS -x SM_NETWORK_INTERFACE_NAME -x SM_HPS -x SM_USER_ENTRY_POINT -x SM_FRAMEWORK_PARAMS -x SM_RESOURCE_CONFIG -x SM_INPUT_DATA_CONFIG -x SM_OUTPUT_DATA_DIR -x SM_CHANNELS -x SM_CURRENT_HOST -x SM_MODULE_NAME -x SM_LOG_LEVEL -x SM_FRAMEWORK_MODULE -x SM_INPUT_DIR -x SM_INPUT_CONFIG_DIR -x SM_OUTPUT_DIR -x SM_NUM_CPUS -x SM_NUM_GPUS -x SM_MODEL_DIR -x SM_MODULE_DIR -x SM_TRAINING_ENV -x SM_USER_ARGS -x SM_OUTPUT_INTERMEDIATE_DIR -x SM_CHANNEL_VALIDATION -x SM_CHANNEL_TRAIN -x SM_HP_BATCH_SIZE -x SM_HP_OTHER_PARA -x SM_HP_MODEL_DIR -x SM_HP_EPOCHS -x SM_HP_LEARNING_RATE -x PYTHONPATH /usr/bin/python3 -m mpi4py horovod_train.py --batch_size 128 --epochs 20 --learning_rate 0.01 --model_dir /opt/ml/model --other_para 0.1\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mWarning: Permanently added 'algo-2,10.0.222.3' (ECDSA) to the list of known hosts.#015\n",
      " Data for JOB [7874,1] offset 0 Total slots allocated 2\n",
      "\n",
      " ========================   JOB MAP   ========================\n",
      "\n",
      " Data for node: ip-10-0-238-206#011Num slots: 1#011Max slots: 0#011Num procs: 1\n",
      " #011Process OMPI jobid: [7874,1] App: 0 Process rank: 0 Bound: UNBOUND\n",
      "\n",
      " Data for node: algo-2#011Num slots: 1#011Max slots: 0#011Num procs: 1\n",
      " #011Process OMPI jobid: [7874,1] App: 0 Process rank: 1 Bound: N/A\n",
      "\n",
      " =============================================================\n",
      " Data for JOB [7874,1] offset 0 Total slots allocated 2\n",
      "\n",
      " ========================   JOB MAP   ========================\n",
      "\n",
      " Data for node: ip-10-0-238-206#011Num slots: 1#011Max slots: 0#011Num procs: 1\n",
      " #011Process OMPI jobid: [7874,1] App: 0 Process rank: 0 Bound: N/A\n",
      "\n",
      " Data for node: algo-2#011Num slots: 1#011Max slots: 0#011Num procs: 1\n",
      " #011Process OMPI jobid: [7874,1] App: 0 Process rank: 1 Bound: UNBOUND\n",
      "\n",
      " =============================================================\u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34mWARNING: Open MPI tried to bind a process but failed.  This is a\u001b[0m\n",
      "\u001b[34mwarning only; your job will continue, though performance may\u001b[0m\n",
      "\u001b[34mbe degraded.\n",
      "\n",
      "  Local host:        ip-10-0-238-206\n",
      "  Application name:  /usr/bin/python3\n",
      "  Error message:     failed to bind memory\n",
      "  Location:          rtc_hwloc.c:447\n",
      "\u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:hvd size:2, hvd rank:1, hvd local_rank:0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:hvd size:2, hvd rank:0, hvd local_rank:0\u001b[0m\n",
      "\u001b[34m[ip-10-0-238-206.us-east-2.compute.internal:00040] 1 more process has sent help message help-orte-odls-default.txt / memory not bound\u001b[0m\n",
      "\u001b[34m[ip-10-0-238-206.us-east-2.compute.internal:00040] Set MCA parameter \"orte_base_help_aggregate\" to 0 to see all help / error messages\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Model: \"sequential\"\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:_________________________________________________________________\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Layer (type)                 Output Shape              Param #   \u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:=================================================================\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:dense (Dense)                (None, 256)               200960    \u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:_________________________________________________________________\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:dense_1 (Dense)              (None, 128)               32896     \u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:_________________________________________________________________\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:dense_2 (Dense)              (None, 64)                8256      \u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:_________________________________________________________________\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:dense_3 (Dense)              (None, 32)                2080      \u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:_________________________________________________________________\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:dense_4 (Dense)              (None, 10)                330       \u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:=================================================================\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Total params: 244,522\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Trainable params: 244,522\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Non-trainable params: 0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:_________________________________________________________________\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:train dataset x shape (128, 784), train dataset y shape (128,)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:train dataset x shape (128, 784), train dataset y shape (128,)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:0#011 step:0#011 loss_mse:0.08999932557344437#011 loss_ce:2.30255126953125#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:0#011 step:100#011 loss_mse:0.02778814360499382#011 loss_ce:0.6566031575202942#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:0#011 step:200#011 loss_mse:0.015084328129887581#011 loss_ce:0.3329394459724426#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:0#011 step:300#011 loss_mse:0.009687108919024467#011 loss_ce:0.2247534692287445#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:accuracy=0.9281;\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:1#011 step:0#011 loss_mse:0.006013757549226284#011 loss_ce:0.12566380202770233#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:1#011 step:100#011 loss_mse:0.008587300777435303#011 loss_ce:0.17940451204776764#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:1#011 step:200#011 loss_mse:0.01052882894873619#011 loss_ce:0.18962937593460083#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:1#011 step:300#011 loss_mse:0.014357161708176136#011 loss_ce:0.2881793975830078#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:accuracy=0.947;\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:2#011 step:0#011 loss_mse:0.010487144812941551#011 loss_ce:0.2902635335922241#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:2#011 step:100#011 loss_mse:0.010596044361591339#011 loss_ce:0.24820786714553833#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:2#011 step:200#011 loss_mse:0.005642630159854889#011 loss_ce:0.14712083339691162#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:2#011 step:300#011 loss_mse:0.007885010913014412#011 loss_ce:0.13919222354888916#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:accuracy=0.9585;\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:3#011 step:0#011 loss_mse:0.012141615152359009#011 loss_ce:0.27224600315093994#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:3#011 step:100#011 loss_mse:0.004883849993348122#011 loss_ce:0.10821634531021118#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:3#011 step:200#011 loss_mse:0.0036493674851953983#011 loss_ce:0.06581586599349976#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:3#011 step:300#011 loss_mse:0.0035524347331374884#011 loss_ce:0.08988851308822632#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:accuracy=0.9562;\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:4#011 step:0#011 loss_mse:0.005969981662929058#011 loss_ce:0.12865543365478516#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:4#011 step:100#011 loss_mse:0.001976214349269867#011 loss_ce:0.04044436663389206#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:4#011 step:200#011 loss_mse:0.0028367736376821995#011 loss_ce:0.05746142566204071#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:4#011 step:300#011 loss_mse:0.0029041958041489124#011 loss_ce:0.08447062969207764#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:accuracy=0.9699;\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:5#011 step:0#011 loss_mse:0.0020668834913522005#011 loss_ce:0.03667929023504257#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:5#011 step:100#011 loss_mse:0.0024585947394371033#011 loss_ce:0.08827000856399536#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:5#011 step:200#011 loss_mse:0.0067876954562962055#011 loss_ce:0.1384744644165039#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:5#011 step:300#011 loss_mse:0.0027182428166270256#011 loss_ce:0.04646357148885727#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:accuracy=0.9658;\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:6#011 step:0#011 loss_mse:0.001825679442845285#011 loss_ce:0.03928185626864433#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:6#011 step:100#011 loss_mse:0.0034950501285493374#011 loss_ce:0.06178978085517883#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:6#011 step:200#011 loss_mse:0.0017436323687434196#011 loss_ce:0.028175726532936096#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:6#011 step:300#011 loss_mse:0.0028033661656081676#011 loss_ce:0.053904030472040176#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:accuracy=0.9673;\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:7#011 step:0#011 loss_mse:0.0030945679172873497#011 loss_ce:0.059958070516586304#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:7#011 step:100#011 loss_mse:0.0029759223107248545#011 loss_ce:0.06354858726263046#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:7#011 step:200#011 loss_mse:0.0015460519352927804#011 loss_ce:0.028971869498491287#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:7#011 step:300#011 loss_mse:0.004603033419698477#011 loss_ce:0.1516030728816986#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:accuracy=0.9702;\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:8#011 step:0#011 loss_mse:0.00034355782554484904#011 loss_ce:0.01000926923006773#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:8#011 step:100#011 loss_mse:0.002147999592125416#011 loss_ce:0.04592825099825859#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:8#011 step:200#011 loss_mse:0.008058984763920307#011 loss_ce:0.16470013558864594#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:8#011 step:300#011 loss_mse:0.005388897843658924#011 loss_ce:0.11556988209486008#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:accuracy=0.9714;\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:9#011 step:0#011 loss_mse:0.0025136619806289673#011 loss_ce:0.09389236569404602#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:9#011 step:100#011 loss_mse:0.0017597631085664034#011 loss_ce:0.05909014493227005#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:9#011 step:200#011 loss_mse:0.002085012849420309#011 loss_ce:0.04665977880358696#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:9#011 step:300#011 loss_mse:0.005036020185798407#011 loss_ce:0.09349408745765686#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:accuracy=0.9696;\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:10#011 step:0#011 loss_mse:0.004387045744806528#011 loss_ce:0.07007843255996704#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:10#011 step:100#011 loss_mse:0.0016716965474188328#011 loss_ce:0.025306597352027893#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:10#011 step:200#011 loss_mse:0.0005185535410419106#011 loss_ce:0.009777537547051907#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:10#011 step:300#011 loss_mse:0.0029150801710784435#011 loss_ce:0.05021066963672638#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:accuracy=0.9671;\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:11#011 step:0#011 loss_mse:0.003730884287506342#011 loss_ce:0.06890799850225449#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:11#011 step:100#011 loss_mse:0.003298862837255001#011 loss_ce:0.09525603801012039#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:11#011 step:200#011 loss_mse:0.003951180260628462#011 loss_ce:0.08622771501541138#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:11#011 step:300#011 loss_mse:0.001442069187760353#011 loss_ce:0.025968004018068314#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:accuracy=0.967;\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:12#011 step:0#011 loss_mse:0.0010483117075636983#011 loss_ce:0.019929541274905205#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:12#011 step:100#011 loss_mse:0.0004039074410684407#011 loss_ce:0.01837870478630066#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:12#011 step:200#011 loss_mse:0.00673773605376482#011 loss_ce:0.15666182339191437#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:12#011 step:300#011 loss_mse:0.0010553154861554503#011 loss_ce:0.019727852195501328#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:accuracy=0.9696;\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:13#011 step:0#011 loss_mse:0.0004899735213257372#011 loss_ce:0.012203515507280827#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:13#011 step:100#011 loss_mse:0.0017445808043703437#011 loss_ce:0.03485827147960663#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:13#011 step:200#011 loss_mse:0.0019543946254998446#011 loss_ce:0.03132268786430359#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:13#011 step:300#011 loss_mse:0.001396052073687315#011 loss_ce:0.025050511583685875#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:accuracy=0.9603;\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:14#011 step:0#011 loss_mse:0.0054016862995922565#011 loss_ce:0.09576468914747238#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:14#011 step:100#011 loss_mse:0.0024175182916224003#011 loss_ce:0.053530253469944#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:14#011 step:200#011 loss_mse:0.003318191971629858#011 loss_ce:0.04968314245343208#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:14#011 step:300#011 loss_mse:0.0004452971916180104#011 loss_ce:0.011854754760861397#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:accuracy=0.9677;\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:15#011 step:0#011 loss_mse:0.0014389905845746398#011 loss_ce:0.027770988643169403#011\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tf_estimator.fit(inputs=data_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------"
     ]
    }
   ],
   "source": [
    "tf_predictor = tf_estimator.deploy(initial_instance_count=1,instance_type='ml.m5.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "验证一下测试数据集的预测结果和真实label的对比\n",
    "\n",
    "通过np.argmax方法得到一个softmax后的输出的预测结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions: \t[7, 2, 1, 0, 4, 1, 4, 7, 6, 9]\n",
      "target values: \t[7, 2, 1, 0, 4, 1, 4, 9, 5, 9]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "results = tf_predictor.predict(test_set[0][:10]/255)['predictions']\n",
    "\n",
    "flat_list = [np.argmax(sublist) for sublist in results]\n",
    "print('predictions: \\t{}'.format(flat_list))\n",
    "print('target values: \\t{}'.format(list(test_set[1][:10])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在定义distributions参数时，还有一个属性控制有多少个process运行training任务，如果要利用到GPU的多卡属性，需要在定义\"processes_per_host\"属性的值等于单机GPU的数量（或者小于单机GPU的数量也可以，只不过有GPU会利用不到，具体容器可见的GPU的数量，是可以在训练代码中绑定的）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#distributions = {'mpi': {'enabled': True, \"processes_per_host\": 2}}\n",
    "distributions = {'mpi': {'enabled': True}}\n",
    "# train_instance_type = 'ml.m5.xlarge'\n",
    "train_instance_type = 'ml.p3.8xlarge'\n",
    "\n",
    "instance_count = 2\n",
    "\n",
    "hyperparameters = {'epochs': 2, 'batch_size': 128, 'learning_rate': 0.01, 'other_para':0.1}\n",
    "\n",
    "tf_estimator_multi_processers = TensorFlow(\n",
    "                       entry_point='horovod_train.py',\n",
    "                       model_dir=model_dir,\n",
    "                       output_path=s3_output_location,\n",
    "                       train_instance_type=train_instance_type,\n",
    "                       train_instance_count=instance_count,\n",
    "                       distributions=distributions,\n",
    "                       hyperparameters=hyperparameters,\n",
    "                       role=sagemaker.get_execution_role(),\n",
    "                       base_job_name='tf-scriptmode-mnist',\n",
    "                       framework_version='2.0.0',\n",
    "                       py_version='py3',\n",
    "                       metric_definitions=metric_definitions,\n",
    "                       script_mode=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-09 09:38:03 Starting - Starting the training job...\n",
      "2020-03-09 09:38:04 Starting - Launching requested ML instances...\n",
      "2020-03-09 09:38:59 Starting - Preparing the instances for training.........\n",
      "2020-03-09 09:40:13 Downloading - Downloading input data...\n",
      "2020-03-09 09:41:01 Training - Training image download completed. Training in progress..\u001b[35m2020-03-09 09:41:05,508 sagemaker-containers INFO     Imported framework sagemaker_tensorflow_container.training\u001b[0m\n",
      "\u001b[35m2020-03-09 09:41:05,918 sagemaker-containers INFO     Starting MPI run as worker node.\u001b[0m\n",
      "\u001b[35m2020-03-09 09:41:05,918 sagemaker-containers INFO     Waiting for MPI Master to create SSH daemon.\u001b[0m\n",
      "\u001b[35m2020-03-09 09:41:05,925 paramiko.transport INFO     Connected (version 2.0, client OpenSSH_7.6p1)\u001b[0m\n",
      "\u001b[35m/usr/local/lib/python3.6/dist-packages/paramiko/kex_ecdh_nist.py:39: CryptographyDeprecationWarning: encode_point has been deprecated on EllipticCurvePublicNumbers and will be removed in a future version. Please use EllipticCurvePublicKey.public_bytes to obtain both compressed and uncompressed point encoding.\n",
      "  m.add_string(self.Q_C.public_numbers().encode_point())\u001b[0m\n",
      "\u001b[35m/usr/local/lib/python3.6/dist-packages/paramiko/kex_ecdh_nist.py:96: CryptographyDeprecationWarning: Support for unsafe construction of public numbers from encoded data will be removed in a future version. Please use EllipticCurvePublicKey.from_encoded_point\n",
      "  self.curve, Q_S_bytes\u001b[0m\n",
      "\u001b[35m/usr/local/lib/python3.6/dist-packages/paramiko/kex_ecdh_nist.py:111: CryptographyDeprecationWarning: encode_point has been deprecated on EllipticCurvePublicNumbers and will be removed in a future version. Please use EllipticCurvePublicKey.public_bytes to obtain both compressed and uncompressed point encoding.\n",
      "  hm.add_string(self.Q_C.public_numbers().encode_point())\u001b[0m\n",
      "\u001b[35m2020-03-09 09:41:06,065 paramiko.transport INFO     Authentication (publickey) successful!\u001b[0m\n",
      "\u001b[35m2020-03-09 09:41:06,066 sagemaker-containers INFO     Can connect to host algo-1\u001b[0m\n",
      "\u001b[35m2020-03-09 09:41:06,067 sagemaker-containers INFO     MPI Master online, creating SSH daemon.\u001b[0m\n",
      "\u001b[35m2020-03-09 09:41:06,074 sagemaker-containers INFO     Waiting for MPI process to finish.\u001b[0m\n",
      "\u001b[34m2020-03-09 09:41:04,970 sagemaker-containers INFO     Imported framework sagemaker_tensorflow_container.training\u001b[0m\n",
      "\u001b[34m2020-03-09 09:41:05,365 sagemaker-containers INFO     Starting MPI run as worker node.\u001b[0m\n",
      "\u001b[34m2020-03-09 09:41:05,366 sagemaker-containers INFO     Creating SSH daemon.\u001b[0m\n",
      "\u001b[34m2020-03-09 09:41:05,372 sagemaker-containers INFO     Waiting for MPI workers to establish their SSH connections\u001b[0m\n",
      "\u001b[34m2020-03-09 09:41:05,373 sagemaker-containers INFO     Cannot connect to host algo-2\u001b[0m\n",
      "\u001b[34m2020-03-09 09:41:05,373 sagemaker-containers INFO     Connection failed with exception: \n",
      " [Errno None] Unable to connect to port 22 on 10.0.139.4\u001b[0m\n",
      "\u001b[34m2020-03-09 09:41:06,380 paramiko.transport INFO     Connected (version 2.0, client OpenSSH_7.6p1)\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/dist-packages/paramiko/kex_ecdh_nist.py:39: CryptographyDeprecationWarning: encode_point has been deprecated on EllipticCurvePublicNumbers and will be removed in a future version. Please use EllipticCurvePublicKey.public_bytes to obtain both compressed and uncompressed point encoding.\n",
      "  m.add_string(self.Q_C.public_numbers().encode_point())\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/dist-packages/paramiko/kex_ecdh_nist.py:96: CryptographyDeprecationWarning: Support for unsafe construction of public numbers from encoded data will be removed in a future version. Please use EllipticCurvePublicKey.from_encoded_point\n",
      "  self.curve, Q_S_bytes\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/dist-packages/paramiko/kex_ecdh_nist.py:111: CryptographyDeprecationWarning: encode_point has been deprecated on EllipticCurvePublicNumbers and will be removed in a future version. Please use EllipticCurvePublicKey.public_bytes to obtain both compressed and uncompressed point encoding.\n",
      "  hm.add_string(self.Q_C.public_numbers().encode_point())\u001b[0m\n",
      "\u001b[34m2020-03-09 09:41:06,510 paramiko.transport INFO     Authentication (publickey) successful!\u001b[0m\n",
      "\u001b[34m2020-03-09 09:41:06,510 sagemaker-containers INFO     Can connect to host algo-2\u001b[0m\n",
      "\u001b[34m2020-03-09 09:41:06,511 sagemaker-containers INFO     Worker algo-2 available for communication\u001b[0m\n",
      "\u001b[34m2020-03-09 09:41:06,511 sagemaker-containers INFO     Env Hosts: ['algo-1', 'algo-2'] Hosts: ['algo-1:4', 'algo-2:4'] process_per_hosts: 4 num_processes: 8\u001b[0m\n",
      "\u001b[34m2020-03-09 09:41:06,513 sagemaker-containers INFO     Network interface name: eth0\u001b[0m\n",
      "\u001b[34m2020-03-09 09:41:06,560 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_mpi_custom_mpi_options\": \"\",\n",
      "        \"sagemaker_mpi_enabled\": true\n",
      "    },\n",
      "    \"channel_input_dirs\": {\n",
      "        \"validation\": \"/opt/ml/input/data/validation\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_tensorflow_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch_size\": 128,\n",
      "        \"other_para\": 0.1,\n",
      "        \"model_dir\": \"/opt/ml/model\",\n",
      "        \"epochs\": 2,\n",
      "        \"learning_rate\": 0.01\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"validation\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"tf-scriptmode-mnist-2020-03-09-09-38-03-377\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-mnist-datasets/tf-scriptmode-mnist-2020-03-09-09-38-03-377/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"horovod_train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 32,\n",
      "    \"num_gpus\": 4,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"horovod_train.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"batch_size\":128,\"epochs\":2,\"learning_rate\":0.01,\"model_dir\":\"/opt/ml/model\",\"other_para\":0.1}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=horovod_train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={\"sagemaker_mpi_custom_mpi_options\":\"\",\"sagemaker_mpi_enabled\":true}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"validation\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\",\"validation\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=horovod_train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_tensorflow_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=32\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=4\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-mnist-datasets/tf-scriptmode-mnist-2020-03-09-09-38-03-377/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_mpi_custom_mpi_options\":\"\",\"sagemaker_mpi_enabled\":true},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\",\"validation\":\"/opt/ml/input/data/validation\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_tensorflow_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"batch_size\":128,\"epochs\":2,\"learning_rate\":0.01,\"model_dir\":\"/opt/ml/model\",\"other_para\":0.1},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"validation\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"tf-scriptmode-mnist-2020-03-09-09-38-03-377\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-mnist-datasets/tf-scriptmode-mnist-2020-03-09-09-38-03-377/source/sourcedir.tar.gz\",\"module_name\":\"horovod_train\",\"network_interface_name\":\"eth0\",\"num_cpus\":32,\"num_gpus\":4,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"horovod_train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--batch_size\",\"128\",\"--epochs\",\"2\",\"--learning_rate\",\"0.01\",\"--model_dir\",\"/opt/ml/model\",\"--other_para\",\"0.1\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_VALIDATION=/opt/ml/input/data/validation\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH_SIZE=128\u001b[0m\n",
      "\u001b[34mSM_HP_OTHER_PARA=0.1\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=2\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=0.01\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/usr/local/bin:/usr/lib/python36.zip:/usr/lib/python3.6:/usr/lib/python3.6/lib-dynload:/usr/local/lib/python3.6/dist-packages:/usr/lib/python3/dist-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34mmpirun --host algo-1:4,algo-2:4 -np 8 --allow-run-as-root --display-map --tag-output -mca btl_tcp_if_include eth0 -mca oob_tcp_if_include eth0 -mca plm_rsh_no_tree_spawn 1 -bind-to socket -map-by slot -mca pml ob1 -mca btl ^openib -mca orte_abort_on_non_zero_status 1 -x NCCL_MIN_NRINGS=4 -x NCCL_SOCKET_IFNAME=eth0 -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH -x PATH -x LD_PRELOAD=/usr/local/lib/python3.6/dist-packages/gethostname.cpython-36m-x86_64-linux-gnu.so -x SM_HOSTS -x SM_NETWORK_INTERFACE_NAME -x SM_HPS -x SM_USER_ENTRY_POINT -x SM_FRAMEWORK_PARAMS -x SM_RESOURCE_CONFIG -x SM_INPUT_DATA_CONFIG -x SM_OUTPUT_DATA_DIR -x SM_CHANNELS -x SM_CURRENT_HOST -x SM_MODULE_NAME -x SM_LOG_LEVEL -x SM_FRAMEWORK_MODULE -x SM_INPUT_DIR -x SM_INPUT_CONFIG_DIR -x SM_OUTPUT_DIR -x SM_NUM_CPUS -x SM_NUM_GPUS -x SM_MODEL_DIR -x SM_MODULE_DIR -x SM_TRAINING_ENV -x SM_USER_ARGS -x SM_OUTPUT_INTERMEDIATE_DIR -x SM_CHANNEL_VALIDATION -x SM_CHANNEL_TRAIN -x SM_HP_BATCH_SIZE -x SM_HP_OTHER_PARA -x SM_HP_MODEL_DIR -x SM_HP_EPOCHS -x SM_HP_LEARNING_RATE -x PYTHONPATH /usr/bin/python3 -m mpi4py horovod_train.py --batch_size 128 --epochs 2 --learning_rate 0.01 --model_dir /opt/ml/model --other_para 0.1\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mWarning: Permanently added 'algo-2,10.0.139.4' (ECDSA) to the list of known hosts.#015\n",
      " Data for JOB [56631,1] offset 0 Total slots allocated 8\n",
      "\n",
      " ========================   JOB MAP   ========================\n",
      "\n",
      " Data for node: ip-10-0-158-108#011Num slots: 4#011Max slots: 0#011Num procs: 4\n",
      " #011Process OMPI jobid: [56631,1] App: 0 Process rank: 0 Bound: UNBOUND\n",
      " #011Process OMPI jobid: [56631,1] App: 0 Process rank: 1 Bound: UNBOUND\n",
      " #011Process OMPI jobid: [56631,1] App: 0 Process rank: 2 Bound: UNBOUND\n",
      " #011Process OMPI jobid: [56631,1] App: 0 Process rank: 3 Bound: UNBOUND\n",
      "\n",
      " Data for node: algo-2#011Num slots: 4#011Max slots: 0#011Num procs: 4\n",
      " #011Process OMPI jobid: [56631,1] App: 0 Process rank: 4 Bound: N/A\n",
      " #011Process OMPI jobid: [56631,1] App: 0 Process rank: 5 Bound: N/A\n",
      " #011Process OMPI jobid: [56631,1] App: 0 Process rank: 6 Bound: N/A\n",
      " #011Process OMPI jobid: [56631,1] App: 0 Process rank: 7 Bound: N/A\n",
      "\n",
      " =============================================================\n",
      " Data for JOB [56631,1] offset 0 Total slots allocated 8\n",
      "\n",
      " ========================   JOB MAP   ========================\n",
      "\n",
      " Data for node: ip-10-0-158-108#011Num slots: 4#011Max slots: 0#011Num procs: 4\n",
      " #011Process OMPI jobid: [56631,1] App: 0 Process rank: 0 Bound: N/A\n",
      " #011Process OMPI jobid: [56631,1] App: 0 Process rank: 1 Bound: N/A\n",
      " #011Process OMPI jobid: [56631,1] App: 0 Process rank: 2 Bound: N/A\n",
      " #011Process OMPI jobid: [56631,1] App: 0 Process rank: 3 Bound: N/A\n",
      "\n",
      " Data for node: algo-2#011Num slots: 4#011Max slots: 0#011Num procs: 4\n",
      " #011Process OMPI jobid: [56631,1] App: 0 Process rank: 4 Bound: UNBOUND\n",
      " #011Process OMPI jobid: [56631,1] App: 0 Process rank: 5 Bound: UNBOUND\n",
      " #011Process OMPI jobid: [56631,1] App: 0 Process rank: 6 Bound: UNBOUND\n",
      " #011Process OMPI jobid: [56631,1] App: 0 Process rank: 7 Bound: UNBOUND\n",
      "\n",
      " =============================================================\u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34mWARNING: Open MPI tried to bind a process but failed.  This is a\u001b[0m\n",
      "\u001b[34mwarning only; your job will continue, though performance may\u001b[0m\n",
      "\u001b[34mbe degraded.\n",
      "\n",
      "  Local host:        ip-10-0-158-108\n",
      "  Application name:  /usr/bin/python3\n",
      "  Error message:     failed to bind memory\n",
      "  Location:          rtc_hwloc.c:447\n",
      "\u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:hvd size:8, hvd rank:2, hvd local_rank:2\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:hvd size:8, hvd rank:6, hvd local_rank:2\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:hvd size:8, hvd rank:4, hvd local_rank:0\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:hvd size:8, hvd rank:5, hvd local_rank:1\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:hvd size:8, hvd rank:3, hvd local_rank:3\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:hvd size:8, hvd rank:0, hvd local_rank:0\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:hvd size:8, hvd rank:7, hvd local_rank:3\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:hvd size:8, hvd rank:1, hvd local_rank:1\u001b[0m\n",
      "\u001b[34m[ip-10-0-158-108.us-east-2.compute.internal:00095] 7 more processes have sent help message help-orte-odls-default.txt / memory not bound\u001b[0m\n",
      "\u001b[34m[ip-10-0-158-108.us-east-2.compute.internal:00095] Set MCA parameter \"orte_base_help_aggregate\" to 0 to see all help / error messages\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:train dataset x shape (128, 784), train dataset y shape (128,)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Model: \"sequential\"\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:_________________________________________________________________\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Layer (type)                 Output Shape              Param #   \u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:=================================================================\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:dense (Dense)                (None, 256)               200960    \u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:_________________________________________________________________\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:dense_1 (Dense)              (None, 128)               32896     \u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:_________________________________________________________________\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:dense_2 (Dense)              (None, 64)                8256      \u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:_________________________________________________________________\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:dense_3 (Dense)              (None, 32)                2080      \u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:_________________________________________________________________\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:dense_4 (Dense)              (None, 10)                330       \u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:=================================================================\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Total params: 244,522\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Trainable params: 244,522\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Non-trainable params: 0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:_________________________________________________________________\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:train dataset x shape (128, 784), train dataset y shape (128,)\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:train dataset x shape (128, 784), train dataset y shape (128,)\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:train dataset x shape (128, 784), train dataset y shape (128,)\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:train dataset x shape (128, 784), train dataset y shape (128,)\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:train dataset x shape (128, 784), train dataset y shape (128,)\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:train dataset x shape (128, 784), train dataset y shape (128,)\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:train dataset x shape (128, 784), train dataset y shape (128,)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:101:367 [0] NCCL INFO NET/Socket : Using [0]eth0:10.0.158.108<0>\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:101:367 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:101:367 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:NCCL version 2.4.7+cuda10.0\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:104:366 [3] NCCL INFO NET/Socket : Using [0]eth0:10.0.158.108<0>\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:103:365 [2] NCCL INFO NET/Socket : Using [0]eth0:10.0.158.108<0>\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:102:374 [1] NCCL INFO NET/Socket : Using [0]eth0:10.0.158.108<0>\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:104:366 [3] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:103:365 [2] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:102:374 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:103:365 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:104:366 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:102:374 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-2:115:379 [2] NCCL INFO NET/Socket : Using [0]eth0:10.0.139.4<0>\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-2:116:386 [3] NCCL INFO NET/Socket : Using [0]eth0:10.0.139.4<0>\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-2:113:378 [0] NCCL INFO NET/Socket : Using [0]eth0:10.0.139.4<0>\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-2:114:377 [1] NCCL INFO NET/Socket : Using [0]eth0:10.0.139.4<0>\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-2:113:378 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-2:115:379 [2] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-2:116:386 [3] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-2:114:377 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-2:116:386 [3] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-2:113:378 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-2:115:379 [2] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-2:114:377 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:103:365 [2] NCCL INFO Setting affinity for GPU 2 to ffffffff\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-2:113:378 [0] NCCL INFO Setting affinity for GPU 0 to ffffffff\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:104:366 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:102:374 [1] NCCL INFO Setting affinity for GPU 1 to ffffffff\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-2:115:379 [2] NCCL INFO Setting affinity for GPU 2 to ffffffff\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-2:116:386 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-2:114:377 [1] NCCL INFO Setting affinity for GPU 1 to ffffffff\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:101:367 [0] NCCL INFO Setting affinity for GPU 0 to ffffffff\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-2:115:379 [2] NCCL INFO CUDA Dev 2[2], Socket NIC distance :  PHB\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-2:116:386 [3] NCCL INFO CUDA Dev 3[3], Socket NIC distance :  PHB\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-2:114:377 [1] NCCL INFO CUDA Dev 1[1], Socket NIC distance :  PHB\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-2:113:378 [0] NCCL INFO CUDA Dev 0[0], Socket NIC distance :  PHB\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:101:367 [0] NCCL INFO CUDA Dev 0[0], Socket NIC distance :  PHB\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:102:374 [1] NCCL INFO CUDA Dev 1[1], Socket NIC distance :  PHB\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:104:366 [3] NCCL INFO CUDA Dev 3[3], Socket NIC distance :  PHB\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:103:365 [2] NCCL INFO CUDA Dev 2[2], Socket NIC distance :  PHB\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-2:114:377 [1] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:102:374 [1] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-2:113:378 [0] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:101:367 [0] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:101:367 [0] NCCL INFO Duplicating rings to 4 per user request.\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:103:365 [2] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:104:366 [3] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-2:116:386 [3] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-2:115:379 [2] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:101:367 [0] NCCL INFO Channel 00 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:101:367 [0] NCCL INFO Channel 01 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:101:367 [0] NCCL INFO Channel 02 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:101:367 [0] NCCL INFO Channel 03 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:101:367 [0] NCCL INFO Ring 00 : 7 -> 0 [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-2:113:378 [0] NCCL INFO Ring 00 : 3 -> 4 [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:101:367 [0] NCCL INFO Ring 00 : 0[0] -> 1[1] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-2:113:378 [0] NCCL INFO Ring 00 : 4[0] -> 5[1] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-2:114:377 [1] NCCL INFO Ring 00 : 5[1] -> 6[2] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:102:374 [1] NCCL INFO Ring 00 : 1[1] -> 2[2] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:103:365 [2] NCCL INFO Ring 00 : 2[2] -> 3[3] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-2:115:379 [2] NCCL INFO Ring 00 : 6[2] -> 7[3] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-2:116:386 [3] NCCL INFO Ring 00 : 7 -> 0 [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:104:366 [3] NCCL INFO Ring 00 : 3 -> 4 [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-2:116:386 [3] NCCL INFO Ring 00 : 7[3] -> 6[2] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:104:366 [3] NCCL INFO Ring 00 : 3[3] -> 2[2] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:101:367 [0] NCCL INFO Ring 00 : 4 -> 0 [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:102:374 [1] NCCL INFO Ring 00 : 1[1] -> 0[0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-2:114:377 [1] NCCL INFO Ring 00 : 5[1] -> 4[0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:103:365 [2] NCCL INFO Ring 00 : 2[2] -> 1[1] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-2:115:379 [2] NCCL INFO Ring 00 : 6[2] -> 5[1] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-2:113:378 [0] NCCL INFO Ring 00 : 4 -> 0 [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-2:113:378 [0] NCCL INFO Ring 00 : 0 -> 4 [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:104:366 [3] NCCL INFO Ring 01 : 3 -> 4 [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-2:116:386 [3] NCCL INFO Ring 01 : 7 -> 0 [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:102:374 [1] NCCL INFO Ring 01 : 1[1] -> 2[2] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-2:114:377 [1] NCCL INFO Ring 01 : 5[1] -> 6[2] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:101:367 [0] NCCL INFO Ring 00 : 0 -> 4 [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-2:115:379 [2] NCCL INFO Ring 01 : 6[2] -> 7[3] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:103:365 [2] NCCL INFO Ring 01 : 2[2] -> 3[3] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:101:367 [0] NCCL INFO Ring 01 : 7 -> 0 [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-2:113:378 [0] NCCL INFO Ring 01 : 3 -> 4 [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:101:367 [0] NCCL INFO Ring 01 : 0[0] -> 1[1] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-2:113:378 [0] NCCL INFO Ring 01 : 4[0] -> 5[1] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-2:116:386 [3] NCCL INFO Ring 01 : 7[3] -> 6[2] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:104:366 [3] NCCL INFO Ring 01 : 3[3] -> 2[2] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-2:115:379 [2] NCCL INFO Ring 01 : 6[2] -> 5[1] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:103:365 [2] NCCL INFO Ring 01 : 2[2] -> 1[1] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:101:367 [0] NCCL INFO Ring 01 : 4 -> 0 [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:102:374 [1] NCCL INFO Ring 01 : 1[1] -> 0[0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-2:114:377 [1] NCCL INFO Ring 01 : 5[1] -> 4[0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-2:113:378 [0] NCCL INFO Ring 01 : 4 -> 0 [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:104:366 [3] NCCL INFO Ring 02 : 3 -> 4 [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-2:116:386 [3] NCCL INFO Ring 02 : 7 -> 0 [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-2:113:378 [0] NCCL INFO Ring 01 : 0 -> 4 [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:103:365 [2] NCCL INFO Ring 02 : 2[2] -> 3[3] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-2:115:379 [2] NCCL INFO Ring 02 : 6[2] -> 7[3] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-2:114:377 [1] NCCL INFO Ring 02 : 5[1] -> 6[2] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:102:374 [1] NCCL INFO Ring 02 : 1[1] -> 2[2] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:101:367 [0] NCCL INFO Ring 01 : 0 -> 4 [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-2:115:379 [2] NCCL INFO Ring 02 : 6[2] -> 5[1] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-2:113:378 [0] NCCL INFO Ring 02 : 3 -> 4 [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:101:367 [0] NCCL INFO Ring 02 : 7 -> 0 [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:103:365 [2] NCCL INFO Ring 02 : 2[2] -> 1[1] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-2:113:378 [0] NCCL INFO Ring 02 : 4[0] -> 5[1] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:101:367 [0] NCCL INFO Ring 02 : 0[0] -> 1[1] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:104:366 [3] NCCL INFO Ring 02 : 3[3] -> 2[2] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-2:116:386 [3] NCCL INFO Ring 02 : 7[3] -> 6[2] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-2:113:378 [0] NCCL INFO Ring 02 : 0 -> 4 [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-2:114:377 [1] NCCL INFO Ring 02 : 5[1] -> 4[0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:102:374 [1] NCCL INFO Ring 02 : 1[1] -> 0[0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-2:116:386 [3] NCCL INFO Ring 03 : 7 -> 0 [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:101:367 [0] NCCL INFO Ring 02 : 0 -> 4 [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-2:115:379 [2] NCCL INFO Ring 03 : 6[2] -> 7[3] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:104:366 [3] NCCL INFO Ring 03 : 3 -> 4 [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:101:367 [0] NCCL INFO Ring 02 : 4 -> 0 [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:103:365 [2] NCCL INFO Ring 03 : 2[2] -> 3[3] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-2:114:377 [1] NCCL INFO Ring 03 : 5[1] -> 6[2] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:102:374 [1] NCCL INFO Ring 03 : 1[1] -> 2[2] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-2:113:378 [0] NCCL INFO Ring 02 : 4 -> 0 [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-2:115:379 [2] NCCL INFO Ring 03 : 6[2] -> 5[1] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-2:113:378 [0] NCCL INFO Ring 03 : 3 -> 4 [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:101:367 [0] NCCL INFO Ring 03 : 7 -> 0 [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:103:365 [2] NCCL INFO Ring 03 : 2[2] -> 1[1] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-2:113:378 [0] NCCL INFO Ring 03 : 4[0] -> 5[1] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:101:367 [0] NCCL INFO Ring 03 : 0[0] -> 1[1] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:104:366 [3] NCCL INFO Ring 03 : 3[3] -> 2[2] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-2:116:386 [3] NCCL INFO Ring 03 : 7[3] -> 6[2] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:104:366 [3] NCCL INFO Trees [0] 2->3->-1/-1/-1 [1] 2->3->-1/-1/-1 [2] 2->3->-1/-1/-1 [3] 2->3->-1/-1/-1\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-2:116:386 [3] NCCL INFO Trees [0] 6->7->-1/-1/-1 [1] 6->7->-1/-1/-1 [2] 6->7->-1/-1/-1 [3] 6->7->-1/-1/-1\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:104:366 [3] NCCL INFO comm 0x7f53d8518180 rank 3 nranks 8 cudaDev 3 nvmlDev 3 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-2:116:386 [3] NCCL INFO comm 0x7fd3e0520100 rank 7 nranks 8 cudaDev 3 nvmlDev 3 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-2:115:379 [2] NCCL INFO Trees [0] 5->6->7/-1/-1 [1] 5->6->7/-1/-1 [2] 5->6->7/-1/-1 [3] 5->6->7/-1/-1\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:103:365 [2] NCCL INFO Trees [0] 1->2->3/-1/-1 [1] 1->2->3/-1/-1 [2] 1->2->3/-1/-1 [3] 1->2->3/-1/-1\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-2:114:377 [1] NCCL INFO Ring 03 : 5[1] -> 4[0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-2:113:378 [0] NCCL INFO Ring 03 : 0 -> 4 [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-2:115:379 [2] NCCL INFO comm 0x7ffa88520250 rank 6 nranks 8 cudaDev 2 nvmlDev 2 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-2:114:377 [1] NCCL INFO Trees [0] 4->5->6/-1/-1 [1] 4->5->6/-1/-1 [2] 4->5->6/-1/-1 [3] 4->5->6/-1/-1\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:102:374 [1] NCCL INFO Ring 03 : 1[1] -> 0[0] via P2P/IPC\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-2:114:377 [1] NCCL INFO comm 0x7f178c51fae0 rank 5 nranks 8 cudaDev 1 nvmlDev 1 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:103:365 [2] NCCL INFO comm 0x7f29b4518440 rank 2 nranks 8 cudaDev 2 nvmlDev 2 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:102:374 [1] NCCL INFO Trees [0] 0->1->2/-1/-1 [1] 0->1->2/-1/-1 [2] 0->1->2/-1/-1 [3] 0->1->2/-1/-1\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:102:374 [1] NCCL INFO comm 0x7f426c517ae0 rank 1 nranks 8 cudaDev 1 nvmlDev 1 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:101:367 [0] NCCL INFO Ring 03 : 0 -> 4 [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:101:367 [0] NCCL INFO Ring 03 : 4 -> 0 [receive] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-2:113:378 [0] NCCL INFO Ring 03 : 4 -> 0 [send] via NET/Socket/0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:101:367 [0] NCCL INFO Trees [0] -1->0->1/4/-1 [1] -1->0->1/4/-1 [2] 4->0->1/-1/-1 [3] 4->0->1/-1/-1\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:101:367 [0] NCCL INFO Using 256 threads, Min Comp Cap 7, Trees enabled for all sizes\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-2:113:378 [0] NCCL INFO Trees [0] 0->4->5/-1/-1 [1] 0->4->5/-1/-1 [2] -1->4->5/0/-1 [3] -1->4->5/0/-1\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:101:367 [0] NCCL INFO comm 0x7ff658540520 rank 0 nranks 8 cudaDev 0 nvmlDev 0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-2:113:378 [0] NCCL INFO comm 0x7fc454518560 rank 4 nranks 8 cudaDev 0 nvmlDev 0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:101:367 [0] NCCL INFO Launch mode Parallel\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 65536, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 65536, errno = 1\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:[algo-2:00114] Read -1, expected 65536, errno = 1\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[algo-2:00115] Read -1, expected 65536, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 4096, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 4096, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:[algo-2:00114] Read -1, expected 4096, errno = 1\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[algo-2:00115] Read -1, expected 4096, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 401408, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 401408, errno = 1\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[algo-2:00115] Read -1, expected 401408, errno = 1\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:[algo-2:00114] Read -1, expected 401408, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 16384, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 16384, errno = 1\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:[algo-2:00114] Read -1, expected 16384, errno = 1\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[algo-2:00115] Read -1, expected 16384, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 16384, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 16384, errno = 1\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[algo-2:00115] Read -1, expected 16384, errno = 1\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:[algo-2:00114] Read -1, expected 16384, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 16384, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 16384, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[algo-2:00115] Read -1, expected 16384, errno = 1\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:[algo-2:00114] Read -1, expected 16384, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 401408, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 401408, errno = 1\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[algo-2:00115] Read -1, expected 401408, errno = 1\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:[algo-2:00114] Read -1, expected 401408, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 4096, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 4096, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 4096, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 4096, errno = 1\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[algo-2:00115] Read -1, expected 4096, errno = 1\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:[algo-2:00114] Read -1, expected 4096, errno = 1\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[algo-2:00115] Read -1, expected 4096, errno = 1\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:[algo-2:00114] Read -1, expected 4096, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:[algo-1:00104] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 8192, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 401408, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 401408, errno = 1\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[algo-2:00115] Read -1, expected 401408, errno = 1\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:[algo-2:00114] Read -1, expected 401408, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 65536, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 65536, errno = 1\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[algo-2:00115] Read -1, expected 65536, errno = 1\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:[algo-2:00114] Read -1, expected 65536, errno = 1\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:[algo-1:00103] Read -1, expected 65536, errno = 1\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:[algo-1:00102] Read -1, expected 65536, errno = 1\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:[algo-2:00115] Read -1, expected 65536, errno = 1\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:[algo-2:00114] Read -1, expected 65536, errno = 1\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:0#011 step:0#011 loss_mse:0.09000144898891449#011 loss_ce:2.302657127380371#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:0#011 step:100#011 loss_mse:0.038569413125514984#011 loss_ce:0.8685925602912903#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:0#011 step:200#011 loss_mse:0.028994135558605194#011 loss_ce:0.6819322109222412#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:0#011 step:300#011 loss_mse:0.017746519297361374#011 loss_ce:0.44616058468818665#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:accuracy=0.8889;\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:1#011 step:0#011 loss_mse:0.01631631888449192#011 loss_ce:0.35693496465682983#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:1#011 step:100#011 loss_mse:0.015294577926397324#011 loss_ce:0.35723891854286194#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:1#011 step:200#011 loss_mse:0.010512507520616055#011 loss_ce:0.23562555015087128#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:1#011 step:300#011 loss_mse:0.0182140301913023#011 loss_ce:0.4419851303100586#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:accuracy=0.8915;\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1781: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:Instructions for updating:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:If using Keras pass *_constraint arguments to layers.\u001b[0m\n",
      "\u001b[34m2020-03-09 09:44:06,670 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\u001b[35m2020-03-09 09:44:36,701 sagemaker-containers INFO     MPI process finished.\u001b[0m\n",
      "\u001b[35m2020-03-09 09:44:36,701 sagemaker_tensorflow_container.training WARNING  No model artifact is saved under path /opt/ml/model. Your training job will not save any model files to S3.\u001b[0m\n",
      "\u001b[35mFor details of how to construct your training script see:\u001b[0m\n",
      "\u001b[35mhttps://sagemaker.readthedocs.io/en/stable/using_tf.html#adapting-your-local-tensorflow-script\u001b[0m\n",
      "\u001b[35m2020-03-09 09:44:36,701 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2020-03-09 09:44:47 Uploading - Uploading generated training model\n",
      "2020-03-09 09:44:47 Completed - Training job completed\n",
      "Training seconds: 548\n",
      "Billable seconds: 548\n",
      "CPU times: user 911 ms, sys: 29.5 ms, total: 941 ms\n",
      "Wall time: 7min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tf_estimator_multi_processers.fit(inputs=data_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable network isolation\n",
    "tf_estimator_isolated = TensorFlow(\n",
    "                       entry_point='horovod_train.py',\n",
    "                       model_dir=model_dir,\n",
    "                       output_path=s3_output_location,\n",
    "                       train_instance_type=train_instance_type,\n",
    "                       train_instance_count=instance_count,\n",
    "                       distributions=distributions,\n",
    "                       hyperparameters=hyperparameters,\n",
    "                       role=sagemaker.get_execution_role(),\n",
    "                       base_job_name='tf-scriptmode-mnist',\n",
    "                       framework_version='2.0.0',\n",
    "                       py_version='py3',\n",
    "                       metric_definitions=metric_definitions,\n",
    "                       script_mode=True,\n",
    "                       enable_network_isolation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-09 03:50:53 Starting - Starting the training job...\n",
      "2020-03-09 03:50:54 Starting - Launching requested ML instances...\n",
      "2020-03-09 03:51:52 Starting - Preparing the instances for training......\n",
      "2020-03-09 03:52:45 Downloading - Downloading input data...\n",
      "2020-03-09 03:53:18 Training - Downloading the training image..\u001b[34m2020-03-09 03:53:35,036 sagemaker-containers INFO     Imported framework sagemaker_tensorflow_container.training\u001b[0m\n",
      "\u001b[34m2020-03-09 03:53:35,042 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-03-09 03:53:35,064 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-03-09 03:53:35,078 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-03-09 03:53:35,088 sagemaker-containers INFO     Starting MPI run as worker node.\u001b[0m\n",
      "\u001b[34m2020-03-09 03:53:35,088 sagemaker-containers INFO     Creating SSH daemon.\u001b[0m\n",
      "\u001b[34m2020-03-09 03:53:35,093 sagemaker-containers INFO     Waiting for MPI workers to establish their SSH connections\u001b[0m\n",
      "\u001b[34m2020-03-09 03:53:35,094 sagemaker-containers INFO     Cannot connect to host algo-2\u001b[0m\n",
      "\u001b[34m2020-03-09 03:53:35,094 sagemaker-containers INFO     Connection failed with exception: \n",
      " [Errno None] Unable to connect to port 22 on 10.0.166.252\u001b[0m\n",
      "\u001b[34m2020-03-09 03:53:36,096 sagemaker-containers INFO     Cannot connect to host algo-2\u001b[0m\n",
      "\u001b[34m2020-03-09 03:53:36,096 sagemaker-containers INFO     Connection failed with exception: \n",
      " [Errno None] Unable to connect to port 22 on 10.0.166.252\u001b[0m\n",
      "\u001b[34m2020-03-09 03:53:37,097 sagemaker-containers INFO     Cannot connect to host algo-2\u001b[0m\n",
      "\u001b[34m2020-03-09 03:53:37,097 sagemaker-containers INFO     Connection failed with exception: \n",
      " [Errno None] Unable to connect to port 22 on 10.0.166.252\u001b[0m\n",
      "\u001b[34m2020-03-09 03:53:38,104 paramiko.transport INFO     Connected (version 2.0, client OpenSSH_7.6p1)\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/dist-packages/paramiko/kex_ecdh_nist.py:39: CryptographyDeprecationWarning: encode_point has been deprecated on EllipticCurvePublicNumbers and will be removed in a future version. Please use EllipticCurvePublicKey.public_bytes to obtain both compressed and uncompressed point encoding.\n",
      "  m.add_string(self.Q_C.public_numbers().encode_point())\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/dist-packages/paramiko/kex_ecdh_nist.py:96: CryptographyDeprecationWarning: Support for unsafe construction of public numbers from encoded data will be removed in a future version. Please use EllipticCurvePublicKey.from_encoded_point\n",
      "  self.curve, Q_S_bytes\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/dist-packages/paramiko/kex_ecdh_nist.py:111: CryptographyDeprecationWarning: encode_point has been deprecated on EllipticCurvePublicNumbers and will be removed in a future version. Please use EllipticCurvePublicKey.public_bytes to obtain both compressed and uncompressed point encoding.\n",
      "  hm.add_string(self.Q_C.public_numbers().encode_point())\u001b[0m\n",
      "\u001b[34m2020-03-09 03:53:38,220 paramiko.transport INFO     Authentication (publickey) successful!\u001b[0m\n",
      "\u001b[34m2020-03-09 03:53:38,220 sagemaker-containers INFO     Can connect to host algo-2\u001b[0m\n",
      "\u001b[34m2020-03-09 03:53:38,221 sagemaker-containers INFO     Worker algo-2 available for communication\u001b[0m\n",
      "\u001b[34m2020-03-09 03:53:38,221 sagemaker-containers INFO     Env Hosts: ['algo-1', 'algo-2'] Hosts: ['algo-1', 'algo-2'] process_per_hosts: 1 num_processes: 2\u001b[0m\n",
      "\u001b[34m2020-03-09 03:53:38,224 sagemaker-containers INFO     Network interface name: eth0\u001b[0m\n",
      "\u001b[34m2020-03-09 03:53:38,230 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-03-09 03:53:38,240 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_mpi_custom_mpi_options\": \"\",\n",
      "        \"sagemaker_mpi_enabled\": true\n",
      "    },\n",
      "    \"channel_input_dirs\": {\n",
      "        \"code\": \"/opt/ml/input/data/code\",\n",
      "        \"validation\": \"/opt/ml/input/data/validation\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_tensorflow_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch_size\": 128,\n",
      "        \"other_para\": 0.1,\n",
      "        \"model_dir\": \"/opt/ml/model\",\n",
      "        \"epochs\": 20,\n",
      "        \"learning_rate\": 0.01\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"code\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"validation\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"tf-scriptmode-mnist-2020-03-09-03-50-53-560\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"/opt/ml/input/data/code/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"horovod_train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"horovod_train.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"batch_size\":128,\"epochs\":20,\"learning_rate\":0.01,\"model_dir\":\"/opt/ml/model\",\"other_para\":0.1}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=horovod_train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={\"sagemaker_mpi_custom_mpi_options\":\"\",\"sagemaker_mpi_enabled\":true}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"code\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"validation\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"code\",\"train\",\"validation\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=horovod_train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_tensorflow_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=/opt/ml/input/data/code/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_mpi_custom_mpi_options\":\"\",\"sagemaker_mpi_enabled\":true},\"channel_input_dirs\":{\"code\":\"/opt/ml/input/data/code\",\"train\":\"/opt/ml/input/data/train\",\"validation\":\"/opt/ml/input/data/validation\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_tensorflow_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"batch_size\":128,\"epochs\":20,\"learning_rate\":0.01,\"model_dir\":\"/opt/ml/model\",\"other_para\":0.1},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"code\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"validation\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"tf-scriptmode-mnist-2020-03-09-03-50-53-560\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"/opt/ml/input/data/code/sourcedir.tar.gz\",\"module_name\":\"horovod_train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"horovod_train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--batch_size\",\"128\",\"--epochs\",\"20\",\"--learning_rate\",\"0.01\",\"--model_dir\",\"/opt/ml/model\",\"--other_para\",\"0.1\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_CODE=/opt/ml/input/data/code\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_VALIDATION=/opt/ml/input/data/validation\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH_SIZE=128\u001b[0m\n",
      "\u001b[34mSM_HP_OTHER_PARA=0.1\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=20\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=0.01\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/usr/local/bin:/usr/lib/python36.zip:/usr/lib/python3.6:/usr/lib/python3.6/lib-dynload:/usr/local/lib/python3.6/dist-packages:/usr/lib/python3/dist-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34mmpirun --host algo-1,algo-2 -np 2 --allow-run-as-root --display-map --tag-output -mca btl_tcp_if_include eth0 -mca oob_tcp_if_include eth0 -mca plm_rsh_no_tree_spawn 1 -bind-to socket -map-by slot -mca pml ob1 -mca btl ^openib -mca orte_abort_on_non_zero_status 1 -x NCCL_MIN_NRINGS=4 -x NCCL_SOCKET_IFNAME=eth0 -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH -x PATH -x LD_PRELOAD=/usr/local/lib/python3.6/dist-packages/gethostname.cpython-36m-x86_64-linux-gnu.so -x SM_HOSTS -x SM_NETWORK_INTERFACE_NAME -x SM_HPS -x SM_USER_ENTRY_POINT -x SM_FRAMEWORK_PARAMS -x SM_RESOURCE_CONFIG -x SM_INPUT_DATA_CONFIG -x SM_OUTPUT_DATA_DIR -x SM_CHANNELS -x SM_CURRENT_HOST -x SM_MODULE_NAME -x SM_LOG_LEVEL -x SM_FRAMEWORK_MODULE -x SM_INPUT_DIR -x SM_INPUT_CONFIG_DIR -x SM_OUTPUT_DIR -x SM_NUM_CPUS -x SM_NUM_GPUS -x SM_MODEL_DIR -x SM_MODULE_DIR -x SM_TRAINING_ENV -x SM_USER_ARGS -x SM_OUTPUT_INTERMEDIATE_DIR -x SM_CHANNEL_CODE -x SM_CHANNEL_VALIDATION -x SM_CHANNEL_TRAIN -x SM_HP_BATCH_SIZE -x SM_HP_OTHER_PARA -x SM_HP_MODEL_DIR -x SM_HP_EPOCHS -x SM_HP_LEARNING_RATE -x PYTHONPATH /usr/bin/python3 -m mpi4py horovod_train.py --batch_size 128 --epochs 20 --learning_rate 0.01 --model_dir /opt/ml/model --other_para 0.1\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mWarning: Permanently added 'algo-2,10.0.166.252' (ECDSA) to the list of known hosts.#015\u001b[0m\n",
      "\u001b[34m Data for JOB [10203,1] offset 0 Total slots allocated 2\n",
      "\n",
      " ========================   JOB MAP   ========================\n",
      "\n",
      " Data for node: ip-10-0-162-73#011Num slots: 1#011Max slots: 0#011Num procs: 1\n",
      " #011Process OMPI jobid: [10203,1] App: 0 Process rank: 0 Bound: UNBOUND\n",
      "\n",
      " Data for node: algo-2#011Num slots: 1#011Max slots: 0#011Num procs: 1\n",
      " #011Process OMPI jobid: [10203,1] App: 0 Process rank: 1 Bound: N/A\n",
      "\n",
      " =============================================================\n",
      " Data for JOB [10203,1] offset 0 Total slots allocated 2\n",
      "\n",
      " ========================   JOB MAP   ========================\n",
      "\n",
      " Data for node: ip-10-0-162-73#011Num slots: 1#011Max slots: 0#011Num procs: 1\n",
      " #011Process OMPI jobid: [10203,1] App: 0 Process rank: 0 Bound: N/A\n",
      "\n",
      " Data for node: algo-2#011Num slots: 1#011Max slots: 0#011Num procs: 1\n",
      " #011Process OMPI jobid: [10203,1] App: 0 Process rank: 1 Bound: UNBOUND\n",
      "\n",
      " =============================================================\u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34mWARNING: Open MPI tried to bind a process but failed.  This is a\u001b[0m\n",
      "\u001b[34mwarning only; your job will continue, though performance may\u001b[0m\n",
      "\u001b[34mbe degraded.\n",
      "\n",
      "  Local host:        ip-10-0-162-73\n",
      "  Application name:  /usr/bin/python3\n",
      "  Error message:     failed to bind memory\n",
      "  Location:          rtc_hwloc.c:447\n",
      "\u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:hvd size:2, hvd rank:0, hvd local_rank:0\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:hvd size:2, hvd rank:1, hvd local_rank:0\u001b[0m\n",
      "\n",
      "2020-03-09 03:53:32 Training - Training image download completed. Training in progress.\u001b[35m2020-03-09 03:53:37,445 sagemaker-containers INFO     Imported framework sagemaker_tensorflow_container.training\u001b[0m\n",
      "\u001b[35m2020-03-09 03:53:37,451 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m2020-03-09 03:53:37,476 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m2020-03-09 03:53:37,492 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m2020-03-09 03:53:37,501 sagemaker-containers INFO     Starting MPI run as worker node.\u001b[0m\n",
      "\u001b[35m2020-03-09 03:53:37,501 sagemaker-containers INFO     Waiting for MPI Master to create SSH daemon.\u001b[0m\n",
      "\u001b[35m2020-03-09 03:53:37,507 paramiko.transport INFO     Connected (version 2.0, client OpenSSH_7.6p1)\u001b[0m\n",
      "\u001b[35m/usr/local/lib/python3.6/dist-packages/paramiko/kex_ecdh_nist.py:39: CryptographyDeprecationWarning: encode_point has been deprecated on EllipticCurvePublicNumbers and will be removed in a future version. Please use EllipticCurvePublicKey.public_bytes to obtain both compressed and uncompressed point encoding.\n",
      "  m.add_string(self.Q_C.public_numbers().encode_point())\u001b[0m\n",
      "\u001b[35m/usr/local/lib/python3.6/dist-packages/paramiko/kex_ecdh_nist.py:96: CryptographyDeprecationWarning: Support for unsafe construction of public numbers from encoded data will be removed in a future version. Please use EllipticCurvePublicKey.from_encoded_point\n",
      "  self.curve, Q_S_bytes\u001b[0m\n",
      "\u001b[35m/usr/local/lib/python3.6/dist-packages/paramiko/kex_ecdh_nist.py:111: CryptographyDeprecationWarning: encode_point has been deprecated on EllipticCurvePublicNumbers and will be removed in a future version. Please use EllipticCurvePublicKey.public_bytes to obtain both compressed and uncompressed point encoding.\n",
      "  hm.add_string(self.Q_C.public_numbers().encode_point())\u001b[0m\n",
      "\u001b[35m2020-03-09 03:53:37,653 paramiko.transport INFO     Authentication (publickey) successful!\u001b[0m\n",
      "\u001b[35m2020-03-09 03:53:37,653 sagemaker-containers INFO     Can connect to host algo-1\u001b[0m\n",
      "\u001b[35m2020-03-09 03:53:37,654 sagemaker-containers INFO     MPI Master online, creating SSH daemon.\u001b[0m\n",
      "\u001b[35m2020-03-09 03:53:37,661 sagemaker-containers INFO     Waiting for MPI process to finish.\u001b[0m\n",
      "\u001b[34m[ip-10-0-162-73.us-east-2.compute.internal:00037] 1 more process has sent help message help-orte-odls-default.txt / memory not bound\u001b[0m\n",
      "\u001b[34m[ip-10-0-162-73.us-east-2.compute.internal:00037] Set MCA parameter \"orte_base_help_aggregate\" to 0 to see all help / error messages\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Model: \"sequential\"\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:_________________________________________________________________\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Layer (type)                 Output Shape              Param #   \u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:=================================================================\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:dense (Dense)                (None, 256)               200960    \u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:_________________________________________________________________\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:dense_1 (Dense)              (None, 128)               32896     \u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:_________________________________________________________________\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:dense_2 (Dense)              (None, 64)                8256      \u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:_________________________________________________________________\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:dense_3 (Dense)              (None, 32)                2080      \u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:_________________________________________________________________\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:dense_4 (Dense)              (None, 10)                330       \u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:=================================================================\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Total params: 244,522\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Trainable params: 244,522\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Non-trainable params: 0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:_________________________________________________________________\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:train dataset x shape (128, 784), train dataset y shape (128,)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:train dataset x shape (128, 784), train dataset y shape (128,)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:0#011 step:0#011 loss_mse:0.09000295400619507#011 loss_ce:2.3027331829071045#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:0#011 step:100#011 loss_mse:0.04556231200695038#011 loss_ce:0.9803677201271057#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:0#011 step:200#011 loss_mse:0.020113924518227577#011 loss_ce:0.4473811089992523#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:0#011 step:300#011 loss_mse:0.01839277893304825#011 loss_ce:0.43038055300712585#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:accuracy=0.9306;\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:1#011 step:0#011 loss_mse:0.016477687284350395#011 loss_ce:0.3199337124824524#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:1#011 step:100#011 loss_mse:0.01239299401640892#011 loss_ce:0.26207512617111206#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:1#011 step:200#011 loss_mse:0.005073928274214268#011 loss_ce:0.12510628998279572#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:1#011 step:300#011 loss_mse:0.007594836410135031#011 loss_ce:0.16586536169052124#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:accuracy=0.9486;\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:2#011 step:0#011 loss_mse:0.00945671834051609#011 loss_ce:0.21051251888275146#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:2#011 step:100#011 loss_mse:0.007670450955629349#011 loss_ce:0.1681361198425293#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:2#011 step:200#011 loss_mse:0.004296272527426481#011 loss_ce:0.09249761700630188#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:2#011 step:300#011 loss_mse:0.004964780993759632#011 loss_ce:0.11417609453201294#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:accuracy=0.9585;\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:3#011 step:0#011 loss_mse:0.003820905927568674#011 loss_ce:0.15631701052188873#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:3#011 step:100#011 loss_mse:0.005495714955031872#011 loss_ce:0.11764917522668839#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:3#011 step:200#011 loss_mse:0.006826203316450119#011 loss_ce:0.15308983623981476#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:3#011 step:300#011 loss_mse:0.007006986066699028#011 loss_ce:0.12169919908046722#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:accuracy=0.9604;\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:4#011 step:0#011 loss_mse:0.004112247843295336#011 loss_ce:0.07832695543766022#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:4#011 step:100#011 loss_mse:0.005501418374478817#011 loss_ce:0.1293511986732483#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:4#011 step:200#011 loss_mse:0.003833096707239747#011 loss_ce:0.0808831974864006#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:4#011 step:300#011 loss_mse:0.003705486422404647#011 loss_ce:0.05898495018482208#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:accuracy=0.9615;\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:5#011 step:0#011 loss_mse:0.0012181808706372976#011 loss_ce:0.027652177959680557#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:5#011 step:100#011 loss_mse:0.0030978568829596043#011 loss_ce:0.09282858669757843#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:5#011 step:200#011 loss_mse:0.006547074764966965#011 loss_ce:0.14023251831531525#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:5#011 step:300#011 loss_mse:0.002810745034366846#011 loss_ce:0.06428798288106918#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:accuracy=0.9675;\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:6#011 step:0#011 loss_mse:0.002939642174169421#011 loss_ce:0.05484271049499512#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:6#011 step:100#011 loss_mse:0.005484937224537134#011 loss_ce:0.10460393875837326#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:6#011 step:200#011 loss_mse:0.0050703356973826885#011 loss_ce:0.11066729575395584#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:6#011 step:300#011 loss_mse:0.0028605065308511257#011 loss_ce:0.08340949565172195#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:accuracy=0.97;\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:7#011 step:0#011 loss_mse:0.0011275792494416237#011 loss_ce:0.021545417606830597#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:7#011 step:100#011 loss_mse:0.004786206409335136#011 loss_ce:0.0824424996972084#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:7#011 step:200#011 loss_mse:0.006499942392110825#011 loss_ce:0.11779892444610596#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:7#011 step:300#011 loss_mse:0.0021252462174743414#011 loss_ce:0.0401274710893631#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:accuracy=0.9703;\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:8#011 step:0#011 loss_mse:0.0013736311811953783#011 loss_ce:0.04198571294546127#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:8#011 step:100#011 loss_mse:0.0021911768708378077#011 loss_ce:0.054047394543886185#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:8#011 step:200#011 loss_mse:0.0020536116790026426#011 loss_ce:0.061995454132556915#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:8#011 step:300#011 loss_mse:0.002195236273109913#011 loss_ce:0.04162755236029625#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:accuracy=0.9715;\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:9#011 step:0#011 loss_mse:0.0007162000983953476#011 loss_ce:0.01528448797762394#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:9#011 step:100#011 loss_mse:0.0028978874906897545#011 loss_ce:0.058288902044296265#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:9#011 step:200#011 loss_mse:0.002440572716295719#011 loss_ce:0.040129102766513824#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:9#011 step:300#011 loss_mse:0.001539847580716014#011 loss_ce:0.04269975423812866#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:accuracy=0.9733;\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:10#011 step:0#011 loss_mse:0.0016190600581467152#011 loss_ce:0.05847347155213356#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:10#011 step:100#011 loss_mse:0.0017165933968499303#011 loss_ce:0.029751919209957123#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:10#011 step:200#011 loss_mse:0.0003024951438419521#011 loss_ce:0.011112701147794724#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:10#011 step:300#011 loss_mse:0.00021731738524977118#011 loss_ce:0.009407289326190948#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:accuracy=0.9694;\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:11#011 step:0#011 loss_mse:4.3042666220571846e-05#011 loss_ce:0.0034519934561103582#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:11#011 step:100#011 loss_mse:0.0005816136253997684#011 loss_ce:0.019582735374569893#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:11#011 step:200#011 loss_mse:0.0011733528226613998#011 loss_ce:0.023342406377196312#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:11#011 step:300#011 loss_mse:0.0019294812809675932#011 loss_ce:0.039448052644729614#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:accuracy=0.9715;\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:12#011 step:0#011 loss_mse:0.0024885903112590313#011 loss_ce:0.04077634960412979#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:12#011 step:100#011 loss_mse:0.0016536311013624072#011 loss_ce:0.024906929582357407#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:12#011 step:200#011 loss_mse:0.0004971891175955534#011 loss_ce:0.015461476519703865#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:12#011 step:300#011 loss_mse:0.006318375468254089#011 loss_ce:0.1621074676513672#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:accuracy=0.9712;\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:13#011 step:0#011 loss_mse:0.0018679796485230327#011 loss_ce:0.03171997517347336#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:13#011 step:100#011 loss_mse:0.006726738065481186#011 loss_ce:0.10605019330978394#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:13#011 step:200#011 loss_mse:0.0038872873410582542#011 loss_ce:0.07320371270179749#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:13#011 step:300#011 loss_mse:0.0008475207141600549#011 loss_ce:0.01703866571187973#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:accuracy=0.9707;\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:14#011 step:0#011 loss_mse:0.00014185858890414238#011 loss_ce:0.0075789946131408215#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:14#011 step:100#011 loss_mse:0.0006552439881488681#011 loss_ce:0.013419458642601967#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:14#011 step:200#011 loss_mse:0.002670427318662405#011 loss_ce:0.14510108530521393#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:14#011 step:300#011 loss_mse:0.0010561805684119463#011 loss_ce:0.030243437737226486#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:accuracy=0.9721;\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:15#011 step:0#011 loss_mse:3.343210482853465e-05#011 loss_ce:0.003080984577536583#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:15#011 step:100#011 loss_mse:0.0019254614599049091#011 loss_ce:0.04199528321623802#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:15#011 step:200#011 loss_mse:0.0008541028364561498#011 loss_ce:0.013570998795330524#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:15#011 step:300#011 loss_mse:0.001722076442092657#011 loss_ce:0.030163897201418877#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:accuracy=0.9698;\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:16#011 step:0#011 loss_mse:0.0004914439632557333#011 loss_ce:0.009565824642777443#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:16#011 step:100#011 loss_mse:8.292055281344801e-05#011 loss_ce:0.00393391540274024#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:16#011 step:200#011 loss_mse:0.0029154098592698574#011 loss_ce:0.04029475152492523#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:16#011 step:300#011 loss_mse:0.0016965465620160103#011 loss_ce:0.026263155043125153#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:accuracy=0.9697;\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:17#011 step:0#011 loss_mse:0.0038396152667701244#011 loss_ce:0.07850056886672974#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:17#011 step:100#011 loss_mse:0.0013719538692384958#011 loss_ce:0.03145790845155716#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:17#011 step:200#011 loss_mse:0.0025588071439415216#011 loss_ce:0.05118013173341751#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:17#011 step:300#011 loss_mse:0.001148237264715135#011 loss_ce:0.023518500849604607#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:accuracy=0.9698;\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:18#011 step:0#011 loss_mse:0.000404366321163252#011 loss_ce:0.008495239540934563#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:18#011 step:100#011 loss_mse:0.0012154269497841597#011 loss_ce:0.018826806917786598#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:18#011 step:200#011 loss_mse:0.0010064583038911223#011 loss_ce:0.014902939088642597#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:18#011 step:300#011 loss_mse:0.003264643717557192#011 loss_ce:0.06897545605897903#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:accuracy=0.9726;\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:19#011 step:0#011 loss_mse:0.002393588423728943#011 loss_ce:0.05791390314698219#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:19#011 step:100#011 loss_mse:0.00015051141963340342#011 loss_ce:0.0048452746123075485#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:19#011 step:200#011 loss_mse:0.001561070792376995#011 loss_ce:0.05357680469751358#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:epoch:19#011 step:300#011 loss_mse:0.0015726467827335#011 loss_ce:0.04700712487101555#011\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:accuracy=0.971;\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1781: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:Instructions for updating:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:If using Keras pass *_constraint arguments to layers.\u001b[0m\n",
      "\u001b[34m2020-03-09 04:04:56,670 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2020-03-09 04:05:27 Uploading - Uploading generated training model\u001b[35m2020-03-09 04:05:26,690 sagemaker-containers INFO     MPI process finished.\u001b[0m\n",
      "\u001b[35m2020-03-09 04:05:26,690 sagemaker_tensorflow_container.training WARNING  No model artifact is saved under path /opt/ml/model. Your training job will not save any model files to S3.\u001b[0m\n",
      "\u001b[35mFor details of how to construct your training script see:\u001b[0m\n",
      "\u001b[35mhttps://sagemaker.readthedocs.io/en/stable/using_tf.html#adapting-your-local-tensorflow-script\u001b[0m\n",
      "\u001b[35m2020-03-09 04:05:26,690 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2020-03-09 04:05:34 Completed - Training job completed\n",
      "Training seconds: 1538\n",
      "Billable seconds: 1538\n",
      "CPU times: user 1.9 s, sys: 72.3 ms, total: 1.97 s\n",
      "Wall time: 15min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tf_estimator_isolated.fit(inputs=data_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
