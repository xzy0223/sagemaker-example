{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5><center><big><b>在AWS上构建云原生推荐模型训练流水线</b></big></center></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 概览\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;在AWS上，您可以通过丰富的服务和工具快速的构建一个自动化的机器学习平台，在这个方案中我们通过使用 AWS Glue(简称Glue) + Amazon Sagemaker（简称Sagemaker） + Step Functions的方式，完成一个serverless机器学习流水线，在这个方案中您不需要配置和维护任何一台EC2，所有的资源都是按需开启和按需付费；在这个方案中，Glue对训练数据进行预处理，Sagemaker完成机器学习的其他环节，包括训练、评估、模型部署等工作，而这些环节通过Step Functions串联成一个工作流。使用这样的方案可以实现模型的整体工程化部署，或者让数据科学家也具有编排自己机器学习工作流的能力，提高模型开发和迭代过程。本实验将向您展示如何通过这些服务构建一个推荐系统中物品embedding和排序模型训练环节的流水线，具体流程图如下：\n",
    "\n",
    "![avatar](stepfunctions_graph_rec_pipeline.png)\n",
    "\n",
    "**实验流程：**\n",
    "- 安装Step Functions Data Scientist SDK和初始化\n",
    "- 分配相应的权限（notebook，step functions，Glue）\n",
    "- 创建Glue ETL Job\n",
    "- 创建Tensorflow Estimatior\n",
    "- 创建并运行Step Functions流水线"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.&nbsp;安装Step Functions Data Scientist SDK和初始化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 安装stepfunction模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/secretstorage/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/opt/conda/lib/python3.7/site-packages/secretstorage/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "Requirement already up-to-date: stepfunctions in /opt/conda/lib/python3.7/site-packages (2.0.0)\n",
      "Requirement already satisfied, skipping upgrade: pyyaml in /opt/conda/lib/python3.7/site-packages (from stepfunctions) (5.3)\n",
      "Requirement already satisfied, skipping upgrade: boto3>=1.14.38 in /opt/conda/lib/python3.7/site-packages (from stepfunctions) (1.17.52)\n",
      "Requirement already satisfied, skipping upgrade: sagemaker>=2.1.0 in /opt/conda/lib/python3.7/site-packages (from stepfunctions) (2.35.0)\n",
      "Requirement already satisfied, skipping upgrade: s3transfer<0.4.0,>=0.3.0 in /opt/conda/lib/python3.7/site-packages (from boto3>=1.14.38->stepfunctions) (0.3.7)\n",
      "Requirement already satisfied, skipping upgrade: botocore<1.21.0,>=1.20.52 in /opt/conda/lib/python3.7/site-packages (from boto3>=1.14.38->stepfunctions) (1.20.52)\n",
      "Requirement already satisfied, skipping upgrade: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.7/site-packages (from boto3>=1.14.38->stepfunctions) (0.10.0)\n",
      "Requirement already satisfied, skipping upgrade: protobuf3-to-dict>=0.1.5 in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.1.0->stepfunctions) (0.1.5)\n",
      "Requirement already satisfied, skipping upgrade: smdebug-rulesconfig==1.0.1 in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.1.0->stepfunctions) (1.0.1)\n",
      "Requirement already satisfied, skipping upgrade: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.1.0->stepfunctions) (20.1)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.1 in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.1.0->stepfunctions) (3.15.8)\n",
      "Requirement already satisfied, skipping upgrade: pandas in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.1.0->stepfunctions) (1.0.1)\n",
      "Requirement already satisfied, skipping upgrade: importlib-metadata>=1.4.0 in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.1.0->stepfunctions) (1.5.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.9.0 in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.1.0->stepfunctions) (1.18.1)\n",
      "Requirement already satisfied, skipping upgrade: google-pasta in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.1.0->stepfunctions) (0.2.0)\n",
      "Requirement already satisfied, skipping upgrade: attrs in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.1.0->stepfunctions) (19.3.0)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.7/site-packages (from botocore<1.21.0,>=1.20.52->boto3>=1.14.38->stepfunctions) (2.8.1)\n",
      "Requirement already satisfied, skipping upgrade: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.7/site-packages (from botocore<1.21.0,>=1.20.52->boto3>=1.14.38->stepfunctions) (1.26.4)\n",
      "Requirement already satisfied, skipping upgrade: six in /opt/conda/lib/python3.7/site-packages (from protobuf3-to-dict>=0.1.5->sagemaker>=2.1.0->stepfunctions) (1.14.0)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->sagemaker>=2.1.0->stepfunctions) (2.4.6)\n",
      "Requirement already satisfied, skipping upgrade: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas->sagemaker>=2.1.0->stepfunctions) (2019.3)\n",
      "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=1.4.0->sagemaker>=2.1.0->stepfunctions) (2.2.0)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install --upgrade stepfunctions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 初始化一些参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import logging\n",
    "import stepfunctions\n",
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "# 通用的初始化\n",
    "stepfunctions.set_stream_logger(level=logging.INFO)\n",
    "\n",
    "bucket = 'video-rec-resources' # 整个实验要使用的bucket\n",
    "source_prefix = 'data/source' # 源数据存放的prefix\n",
    "output_prefix = 'data/output' # 转换完的数据存放的prefix\n",
    "saved_model_prefix = 'model/output' # 模型存储的位置\n",
    "\n",
    "# 生成uuid，用于唯一化各个组件需要用到的name\n",
    "id = uuid.uuid4().hex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.&nbsp;分配相应的权限：\n",
    "\n",
    "#### I.给notebook的role分配权限，使其可以创建step function的各个组件\n",
    "给sagemaker notebook的role增加`AWSStepFunctionsFullAccess`权限，以便可以在notebook中创建step function的工作流\n",
    "\n",
    "#### II.给notebook的role分配权限，使其可以创建Glue Job\n",
    "- 找到notebook的Role -> Permission -> 选择某条策略 -> edit policy\n",
    "- Add additional Policy -> Service选择**Glue** -> Action选择**Write** -> Resource选择**all resource**\n",
    "- Review and Save changes\n",
    "\n",
    "#### III.给StepFunction创建IAM Role，使其未来可以具有操作sagemaker的权限\n",
    "- 进入IAM控制台 -> Role -> Create Rule\n",
    "- trusted entity选择**AWS Service** -> 服务选择**Step Function** -> Next Permission\n",
    "- 一路Next直到输入名称`StepFunctionsWorkflowExecutionRole` -> **Create**\n",
    "\n",
    "下面将给这个Role赋予可以操作sagemaker和EventBridge创建event rules的权限，遵从最佳实践--最小化权限原则\n",
    "\n",
    "- 在Permission下 -> Attach Policies -> Create Policy\n",
    "- 粘贴如下的Policy，并替换必要的变量 [YOUR_NOTEBOOK_ROLE_ARN]， [YOUR_GLUE_ETL_JOB_PREFIX]；由于glue job的名字有动态的后缀，所以这里只需要定义好前缀。\n",
    "- [YOUR_GLUE_ETL_JOB_PREFIX] = glue-mnist-etl\n",
    "- Review -> 输入名字：StepFunctionsWorkflowExecutionPolicy，并创建Policy\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": \"iam:PassRole\",\n",
    "            \"Resource\": \"[YOUR_NOTEBOOK_ROLE_ARN]\",\n",
    "            \"Condition\": {\n",
    "                \"StringEquals\": {\n",
    "                    \"iam:PassedToService\": \"sagemaker.amazonaws.com\"\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"sagemaker:CreateModel\",\n",
    "                \"sagemaker:DeleteEndpointConfig\",\n",
    "                \"sagemaker:DescribeTrainingJob\",\n",
    "                \"sagemaker:CreateEndpoint\",\n",
    "                \"sagemaker:StopTrainingJob\",\n",
    "                \"sagemaker:CreateTrainingJob\",\n",
    "                \"sagemaker:UpdateEndpoint\",\n",
    "                \"sagemaker:CreateEndpointConfig\",\n",
    "                \"sagemaker:DeleteEndpoint\"\n",
    "            ],\n",
    "            \"Resource\": [\n",
    "                \"arn:aws:sagemaker:*:*:*\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"events:DescribeRule\",\n",
    "                \"events:PutRule\",\n",
    "                \"events:PutTargets\"\n",
    "            ],\n",
    "            \"Resource\": [\n",
    "                \"arn:aws:events:*:*:rule/StepFunctionsGetEventsForSageMakerTrainingJobsRule\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"glue:StartJobRun\",\n",
    "                \"glue:GetJobRun\",\n",
    "                \"glue:BatchStopJobRun\",\n",
    "                \"glue:GetJobRuns\"\n",
    "            ],\n",
    "            \"Resource\": \"arn:aws:glue:*:*:job/[YOUR_GLUE_ETL_JOB_PREFIX]*\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "- 然后返回给Role attach policy的窗口，选择刚刚创建的Policy，并attach\n",
    "\n",
    "#### IIII.创建Glue Job要使用的Role，这个Role要有Glue Job的要读写数据的Bucket的权限\n",
    "\n",
    "- 进入IAM控制台 -> Roles -> Create Role\n",
    "- trusted entity选择**AWS Service** -> 服务选择**Glue** -> **Next Permission**\n",
    "- 选择 `AmazonS3FullAccess policy`，然后一路next\n",
    "- 直到Review页面，属于名称 `AWS-Glue-S3-Bucket-Access` -> **Create Role**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.&nbsp;创建Glue ETL Job\n",
    "\n",
    "- 在这里我们创建的Glue ETL Job的作用是处理movie数据集和user对movie rating数据集生成模型的训练数据\n",
    "- 此外还会创建另外一个Glue ETL Job用于生成电影的item2vec embeding数据，embedding召回层重要的一路召回来源\n",
    "- glue是一个serverless的etl服务，底层通过spark实现，我们可以编写etl脚本交由glue运行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.s3 import S3Uploader\n",
    "\n",
    "# 创建调用sagemaker需要的session\n",
    "session = sagemaker.Session()\n",
    "\n",
    "# 将生成训练数据的glue脚本上传到s3\n",
    "glue_script_location = S3Uploader.upload(local_path='./glue_script/training_sample.py',\n",
    "                               desired_s3_uri='s3://{}/{}'.format(bucket, 'glue_script'),\n",
    "                               sagemaker_session=session)\n",
    "\n",
    "job_name = 'training-sample-job-{}'.format(id) # 定义glue job的名字\n",
    "glue_role = 'AWS-Glue-S3-Bucket-Access'  # 使用权限设置章节中创建的glue role\n",
    "\n",
    "glue_client = boto3.client('glue')\n",
    "\n",
    "# 创建glue etl job\n",
    "response = glue_client.create_job(\n",
    "    Name=job_name,\n",
    "    Description='PySpark job to generate training and validation data',\n",
    "    Role=glue_role, \n",
    "    ExecutionProperty={\n",
    "        'MaxConcurrentRuns': 2\n",
    "    },\n",
    "    Command={\n",
    "        'Name': 'glueetl',\n",
    "        'ScriptLocation': glue_script_location,\n",
    "        'PythonVersion': '3'\n",
    "    },\n",
    "    DefaultArguments={\n",
    "        '--job-language': 'python'\n",
    "    },\n",
    "    GlueVersion='2.0',\n",
    "    WorkerType='Standard',\n",
    "    NumberOfWorkers=2,\n",
    "    Timeout=60\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将生成embedding数据的glue脚本上传到s3\n",
    "glue_script_location = S3Uploader.upload(local_path='./glue_script/item2vec_embedding.py',\n",
    "                               desired_s3_uri='s3://{}/{}'.format(bucket, 'glue_script'),\n",
    "                               sagemaker_session=session)\n",
    "\n",
    "job_name = 'item2vec-embedding-job-{}'.format(id) # 定义glue job的名字\n",
    "\n",
    "# 创建glue etl job\n",
    "response = glue_client.create_job(\n",
    "    Name=job_name,\n",
    "    Description='PySpark job to generate training and validation data',\n",
    "    Role=glue_role, \n",
    "    ExecutionProperty={\n",
    "        'MaxConcurrentRuns': 2\n",
    "    },\n",
    "    Command={\n",
    "        'Name': 'glueetl',\n",
    "        'ScriptLocation': glue_script_location,\n",
    "        'PythonVersion': '3'\n",
    "    },\n",
    "    DefaultArguments={\n",
    "        '--job-language': 'python'\n",
    "    },\n",
    "    GlueVersion='2.0',\n",
    "    WorkerType='Standard',\n",
    "    NumberOfWorkers=2,\n",
    "    Timeout=60\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.&nbsp;创建Tensorflow Estimatior\n",
    "\n",
    "- estimator是一个对象，用来完成sagemaker中的各个功能，training和hosting等，针对不同的框架有不同的Estimator类\n",
    "- 需要定义estimator的配置，比如训练数据，训练实例类型，超参数等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "# 定义训练配置，实例类型和超参等\n",
    "s3_output_location = 's3://{}/{}'.format(bucket, saved_model_prefix)\n",
    "model_dir = '/opt/ml/model'\n",
    "train_instance_type = 'ml.m5.xlarge'\n",
    "hyperparameters = {'epochs': 1, 'batch_size': 12, 'learning_rate': 0.001}\n",
    "\n",
    "# 如果需要监控训练算法中某一个指标，可以定义metric_definitions并传入Tensorflow estimator\n",
    "# 被监控的metrics会被解析并打到cloudwatch\n",
    "metric_definitions = [\n",
    "    {\n",
    "        'Name': 'accuracy',\n",
    "        'Regex': 'accuracy:\\s([0-1].[0-9]*)'\n",
    "    },\n",
    "    {\n",
    "        'Name': 'roc_auc',\n",
    "        'Regex': 'auc:\\s([0-1].[0-9]*)'\n",
    "    },\n",
    "    {\n",
    "        'Name': 'pr_auc',\n",
    "        'Regex': 'auc_1:\\s([0-1].[0-9]*)'\n",
    "    }\n",
    "]\n",
    "\n",
    "# 创建一个tensorflow的estimator\n",
    "tf_estimator = TensorFlow(\n",
    "    entry_point='./tf_model/WideNDeep-sm.py',\n",
    "    model_dir=model_dir,\n",
    "    output_path=s3_output_location,\n",
    "    instance_type=train_instance_type,\n",
    "    instance_count=1,\n",
    "    hyperparameters=hyperparameters,\n",
    "    role=sagemaker.get_execution_role(),\n",
    "    base_job_name='tf-scriptmode-rec',\n",
    "    framework_version='2.3.0',\n",
    "    py_version='py37',\n",
    "    enable_sagemaker_metrics=True,\n",
    "    metric_definitions=metric_definitions,\n",
    "    script_mode=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.&nbsp;创建并运行Step Functions流水线\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;Step Functions是AWS的任务编排服务，在其中最核心的概念就是Step，也就是工作流中每一步要执行的任务；另外Step Functions中每个step都会有input和output；并且可以在Step Functions中编排复杂的任务逻辑，比如并行、判断、分支等等，在这个实验中我们使用最简单的串行逻辑，按照数据处理、模型训练、模型创建到模型部署的流程顺序执行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### import相关module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stepfunctions\n",
    "from stepfunctions import steps\n",
    "from stepfunctions.steps import TrainingStep, ModelStep\n",
    "from stepfunctions.inputs import ExecutionInput\n",
    "from stepfunctions.workflow import Workflow\n",
    "from stepfunctions.steps import Parallel, Choice, ChoiceRule, Fail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义step function的input的schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义传入step functions的input schema\n",
    "execution_input = ExecutionInput(schema={\n",
    "    'TrainingJobName': str,\n",
    "    'GlueJobName': str,\n",
    "    'ModelName': str,\n",
    "    'EndpointName': str,\n",
    "    'SourcePath': str,\n",
    "    'OutputPath': str,\n",
    "    'LambdaName': str\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义glue step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建生成训练数据的glue etl job step\n",
    "training_data_etl_step = steps.GlueStartJobRunStep(\n",
    "    'Generate training data for Rec Model',\n",
    "    parameters={\"JobName\": execution_input['GlueJobName'],\n",
    "                \"Arguments\":{\n",
    "                    '--SOURCE_PATH': execution_input['SourcePath'],\n",
    "                    '--OUTPUT_PATH': execution_input['OutputPath'],\n",
    "                    }\n",
    "               }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义sagemaker training step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义训练数据的位置\n",
    "train_data = 's3://{}/{}/{}'.format(bucket, output_prefix, 'sampledata/trainingSamples')\n",
    "validation_data = 's3://{}/{}/{}'.format(bucket, output_prefix, 'sampledata/testSamples')\n",
    "\n",
    "# data chennels会作为参数传递给estimator构造函数，定义训练数据的信息\n",
    "data_channels = {'train': train_data, 'validation': validation_data}\n",
    "\n",
    "# 创建模型训练的step\n",
    "training_step = steps.TrainingStep(\n",
    "    'Model Training', \n",
    "    estimator=tf_estimator,\n",
    "    data=data_channels,\n",
    "    job_name=execution_input['TrainingJobName'],\n",
    "    wait_for_completion=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义sagemaker生成model的step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型训练结束后需要创建sm中的模型对象，因此创建对应的step\n",
    "model_step = steps.ModelStep(\n",
    "    'Save Model',\n",
    "    model=training_step.get_expected_model(),\n",
    "    model_name=execution_input['ModelName'],\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    result_path='$.ModelStepResults'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义部署model的endpoint configure的step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 如果要对模型进行部署，需要定义模型部署的endpiont的配置，因此创建对应的step\n",
    "endpoint_config_step = steps.EndpointConfigStep(\n",
    "    \"Create Model Endpoint Config\",\n",
    "    endpoint_config_name=execution_input['ModelName'],\n",
    "    model_name=execution_input['ModelName'],\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m4.xlarge'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 创建endpoint step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建完配置后就需要真正的部署endpoint，创建对应的step\n",
    "endpoint_step = steps.EndpointStep(\n",
    "    'Update Model Endpoint',\n",
    "    endpoint_name=execution_input['EndpointName'],\n",
    "    endpoint_config_name=execution_input['ModelName'],\n",
    "    update=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 创建lambda step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在流程图中，有一步需要检查模型训练的metric是否满足业务需求（准确率大于90%）\n",
    "# 这个需要一个lambda实现，因此需要创建一个lambda函数用于获取训练任务的metrics\n",
    "lambda_step = steps.compute.LambdaStep(\n",
    "    'Query Training Results',\n",
    "    parameters={  \n",
    "        \"FunctionName\": execution_input['LambdaName'],\n",
    "        'Payload':{\n",
    "            \"TrainingJobName.$\": '$.TrainingJobName'\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 创建Choice State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Update Model Endpoint EndpointStep(resource='arn:aws:states:::sagemaker:createEndpoint', parameters={'EndpointConfigName': <stepfunctions.inputs.placeholders.ExecutionInput object at 0x7f707daf2a10>, 'EndpointName': <stepfunctions.inputs.placeholders.ExecutionInput object at 0x7f707daf2890>}, type='Task')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 我们需要根据lambda step打出来的metrics结果判断是否要进行之后模型部署的阶段\n",
    "# 所以我们需要创建一个choice state用于进行条件判断\n",
    "check_accuracy_step = steps.states.Choice(\n",
    "    'Accuracy > 90%'\n",
    ")\n",
    "\n",
    "# 如果metrics不满足条件，那么整个流水线失败，需要创建一个Fail state\n",
    "fail_step = steps.states.Fail(\n",
    "    'Model Accuracy Too Low',\n",
    "    comment='Validation accuracy lower than threshold'\n",
    ")\n",
    "\n",
    "# 在Choice state中需要定义rule，用于判断，从而选择下一步step是哪个\n",
    "threshold_rule = steps.choice_rule.ChoiceRule.NumericGreaterThan(variable=lambda_step.output()['Payload']['trainingMetrics'][0]['Value'], value=.9)\n",
    "\n",
    "# 添加choice，如果这个rule为真那么就走endpoint_config_step，否则就是fail state\n",
    "check_accuracy_step.add_choice(rule=threshold_rule, next_step=endpoint_config_step)\n",
    "check_accuracy_step.default_choice(next_step=fail_step)\n",
    "\n",
    "# 还需要定义endpoint_config_step之后是哪一步\n",
    "endpoint_config_step.next(endpoint_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 生成workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将整个模型训练的流程串联起来\n",
    "ml_chain = steps.Chain([\n",
    "    training_data_etl_step,\n",
    "    training_step,\n",
    "    model_step,\n",
    "    lambda_step,\n",
    "    check_accuracy_step\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建另外一个embedding的glue etl step\n",
    "embedding_etl_step = steps.GlueStartJobRunStep(\n",
    "    'item2vec embedding',\n",
    "    parameters={\"JobName\": execution_input['GlueJobName'],\n",
    "                \"Arguments\":{\n",
    "                    '--SOURCE_PATH': execution_input['SourcePath'],\n",
    "                    '--OUTPUT_PATH': execution_input['OutputPath'],\n",
    "                    }\n",
    "               }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在流程图中我们需要在数据准备好后同时开始两条处理流程，因此需要创建一个parallel step\n",
    "parallel_step = Parallel('parallel line inclueds model training and embedding')\n",
    "\n",
    "# 在parallel step中分别添加两条流程，可以接收chain或者step作为参数\n",
    "parallel_step.add_branch(ml_chain)\n",
    "parallel_step.add_branch(embedding_etl_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用之前创建的step function role\n",
    "workflow_execution_role = 'arn:aws:iam::935206693453:role/StepFunctionsWorkflowExecutionRole'\n",
    "\n",
    "# 配置workflow\n",
    "workflow = Workflow(\n",
    "    name='My-SM-Pipline-{}'.format(id),\n",
    "    definition=parallel_step,\n",
    "    role=workflow_execution_role,\n",
    "    execution_input=execution_input\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://do0of8uwbahzz.cloudfront.net/graph.css\">\n",
       "<div id=\"graph-371\" class=\"workflowgraph\">\n",
       "    \n",
       "    <svg></svg>\n",
       "    \n",
       "</div>\n",
       "\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "require.config({\n",
       "    paths: {\n",
       "        sfn: \"https://do0of8uwbahzz.cloudfront.net/sfn\",\n",
       "    }\n",
       "});\n",
       "\n",
       "require(['sfn'], function(sfn) {\n",
       "    var element = document.getElementById('graph-371')\n",
       "\n",
       "    var options = {\n",
       "        width: parseFloat(getComputedStyle(element, null).width.replace(\"px\", \"\")),\n",
       "        height: 600,\n",
       "        layout: 'LR',\n",
       "        resizeHeight: true\n",
       "    };\n",
       "\n",
       "    var definition = {\"StartAt\": \"parallel line inclueds model training and embedding\", \"States\": {\"parallel line inclueds model training and embedding\": {\"Type\": \"Parallel\", \"End\": true, \"Branches\": [{\"StartAt\": \"Generate training data for Rec Model\", \"States\": {\"Generate training data for Rec Model\": {\"Parameters\": {\"JobName.$\": \"$$.Execution.Input['GlueJobName']\", \"Arguments\": {\"--SOURCE_PATH.$\": \"$$.Execution.Input['SourcePath']\", \"--OUTPUT_PATH.$\": \"$$.Execution.Input['OutputPath']\"}}, \"Resource\": \"arn:aws:states:::glue:startJobRun.sync\", \"Type\": \"Task\", \"Next\": \"Model Training\"}, \"Model Training\": {\"Resource\": \"arn:aws:states:::sagemaker:createTrainingJob.sync\", \"Parameters\": {\"AlgorithmSpecification\": {\"TrainingImage\": \"763104351884.dkr.ecr.us-west-2.amazonaws.com/tensorflow-training:2.3.0-cpu-py37\", \"TrainingInputMode\": \"File\", \"MetricDefinitions\": [{\"Name\": \"accuracy\", \"Regex\": \"accuracy:\\\\s([0-1].[0-9]*)\"}, {\"Name\": \"roc_auc\", \"Regex\": \"auc:\\\\s([0-1].[0-9]*)\"}, {\"Name\": \"pr_auc\", \"Regex\": \"auc_1:\\\\s([0-1].[0-9]*)\"}]}, \"OutputDataConfig\": {\"S3OutputPath\": \"s3://video-rec-resources/model/output\"}, \"StoppingCondition\": {\"MaxRuntimeInSeconds\": 86400}, \"ResourceConfig\": {\"InstanceCount\": 1, \"InstanceType\": \"ml.m5.xlarge\", \"VolumeSizeInGB\": 30}, \"RoleArn\": \"arn:aws:iam::935206693453:role/service-role/AmazonSageMaker-ExecutionRole-20210412T162678\", \"InputDataConfig\": [{\"DataSource\": {\"S3DataSource\": {\"S3DataType\": \"S3Prefix\", \"S3Uri\": \"s3://video-rec-resources/data/output/sampledata/trainingSamples\", \"S3DataDistributionType\": \"FullyReplicated\"}}, \"ChannelName\": \"train\"}, {\"DataSource\": {\"S3DataSource\": {\"S3DataType\": \"S3Prefix\", \"S3Uri\": \"s3://video-rec-resources/data/output/sampledata/testSamples\", \"S3DataDistributionType\": \"FullyReplicated\"}}, \"ChannelName\": \"validation\"}], \"HyperParameters\": {\"epochs\": \"1\", \"batch_size\": \"12\", \"learning_rate\": \"0.001\", \"sagemaker_submit_directory\": \"\\\"s3://video-rec-resources/tf-scriptmode-rec-2021-04-24-14-55-56-395/source/sourcedir.tar.gz\\\"\", \"sagemaker_program\": \"\\\"WideNDeep-sm.py\\\"\", \"sagemaker_container_log_level\": \"20\", \"sagemaker_job_name\": \"\\\"tf-scriptmode-rec-2021-04-24-14-55-56-493\\\"\", \"sagemaker_region\": \"\\\"us-west-2\\\"\", \"model_dir\": \"\\\"/opt/ml/model\\\"\"}, \"TrainingJobName.$\": \"$$.Execution.Input['TrainingJobName']\", \"DebugHookConfig\": {\"S3OutputPath\": \"s3://video-rec-resources/model/output\"}}, \"Type\": \"Task\", \"Next\": \"Save Model\"}, \"Save Model\": {\"ResultPath\": \"$.ModelStepResults\", \"Parameters\": {\"ModelName.$\": \"$$.Execution.Input['ModelName']\", \"PrimaryContainer\": {\"Image\": \"763104351884.dkr.ecr.us-west-2.amazonaws.com/tensorflow-inference:2.3.0-cpu\", \"Environment\": {\"SAGEMAKER_PROGRAM\": null, \"SAGEMAKER_SUBMIT_DIRECTORY\": null, \"SAGEMAKER_CONTAINER_LOG_LEVEL\": \"20\", \"SAGEMAKER_REGION\": \"us-west-2\"}, \"ModelDataUrl.$\": \"$['ModelArtifacts']['S3ModelArtifacts']\"}, \"ExecutionRoleArn\": \"arn:aws:iam::935206693453:role/service-role/AmazonSageMaker-ExecutionRole-20210412T162678\"}, \"Resource\": \"arn:aws:states:::sagemaker:createModel\", \"Type\": \"Task\", \"Next\": \"Query Training Results\"}, \"Query Training Results\": {\"Parameters\": {\"FunctionName.$\": \"$$.Execution.Input['LambdaName']\", \"Payload\": {\"TrainingJobName.$\": \"$.TrainingJobName\"}}, \"Resource\": \"arn:aws:states:::lambda:invoke\", \"Type\": \"Task\", \"Next\": \"Accuracy > 90%\"}, \"Accuracy > 90%\": {\"Type\": \"Choice\", \"Choices\": [{\"Variable\": \"$['Payload']['trainingMetrics'][0]['Value']\", \"NumericGreaterThan\": 0.9, \"Next\": \"Create Model Endpoint Config\"}], \"Default\": \"Model Accuracy Too Low\"}, \"Model Accuracy Too Low\": {\"Comment\": \"Validation accuracy lower than threshold\", \"Type\": \"Fail\"}, \"Create Model Endpoint Config\": {\"Resource\": \"arn:aws:states:::sagemaker:createEndpointConfig\", \"Parameters\": {\"EndpointConfigName.$\": \"$$.Execution.Input['ModelName']\", \"ProductionVariants\": [{\"InitialInstanceCount\": 1, \"InstanceType\": \"ml.m4.xlarge\", \"ModelName.$\": \"$$.Execution.Input['ModelName']\", \"VariantName\": \"AllTraffic\"}]}, \"Type\": \"Task\", \"Next\": \"Update Model Endpoint\"}, \"Update Model Endpoint\": {\"Resource\": \"arn:aws:states:::sagemaker:createEndpoint\", \"Parameters\": {\"EndpointConfigName.$\": \"$$.Execution.Input['ModelName']\", \"EndpointName.$\": \"$$.Execution.Input['EndpointName']\"}, \"Type\": \"Task\", \"End\": true}}}, {\"StartAt\": \"item2vec embedding\", \"States\": {\"item2vec embedding\": {\"Parameters\": {\"JobName.$\": \"$$.Execution.Input['GlueJobName']\", \"Arguments\": {\"--SOURCE_PATH.$\": \"$$.Execution.Input['SourcePath']\", \"--OUTPUT_PATH.$\": \"$$.Execution.Input['OutputPath']\"}}, \"Resource\": \"arn:aws:states:::glue:startJobRun.sync\", \"Type\": \"Task\", \"End\": true}}}]}}};\n",
       "    var elementId = '#graph-371';\n",
       "\n",
       "    var graph = new sfn.StateMachineGraph(definition, elementId, options);\n",
       "    graph.render();\n",
       "});\n",
       "\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow.render_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[INFO] Workflow updated successfully on AWS Step Functions. All execute() calls will use the updated definition and role within a few seconds. \u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'arn:aws:states:us-west-2:935206693453:stateMachine:My-SM-Pipline-a9ffe923de7b42fba35f2ea6b34f6a42'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建workflow\n",
    "# workflow.create()\n",
    "\n",
    "# 更新现有的workflow\n",
    "state_machine_arn = 'arn:aws:states:us-west-2:935206693453:stateMachine:My-SM-Pipline-a9ffe923de7b42fba35f2ea6b34f6a42'\n",
    "workflow = Workflow.attach(state_machine_arn)\n",
    "workflow.update(parallel_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 执行workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[INFO] Workflow execution started successfully on AWS Step Functions.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# 定义要传入glue job的参数\n",
    "source_path = 's3://{}/{}/'.format(bucket, source_prefix)\n",
    "output_path = 's3://{}/{}/'.format(bucket, output_prefix)\n",
    "# 定义lambda step的lambda函数名\n",
    "lambda_name = 'query-training-metrics'\n",
    "\n",
    "# 执行workflow流程\n",
    "execution = workflow.execute(\n",
    "    inputs={\n",
    "        'TrainingJobName': 'my-sm-pipeline-job-{}'.format(id), # Each Sagemaker Job requires a unique name,\n",
    "        'GlueJobName': job_name,\n",
    "        'ModelName': 'my-sm-pipeline-model-{}'.format(id),\n",
    "        'EndpointName': 'my-sm-pipeline-endpoint-{}'.format(id),\n",
    "        'SourcePath': source_path,\n",
    "        'OutputPath': output_path,\n",
    "        'LambdaName': lambda_name\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
